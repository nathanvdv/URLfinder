{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.2.2)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: orjson>=3.10.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from click>=8.1.7->duckduckgo_search->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                                  OfficialName  \\\n",
      "0  0201.310.929                                                           IGL   \n",
      "1  0202.239.951                                                      PROXIMUS   \n",
      "2  0203.201.340                                     Nationale Bank van België   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het Land van Waas   \n",
      "4  0206.653.946       Rijksinstituut voor Ziekte- en Invaliditeitsverzekering   \n",
      "\n",
      "  ZipCode         Municipality                Street HouseNumber  \\\n",
      "0    3600                 Genk            Klotstraat         125   \n",
      "1    1030           Schaarbeek  Koning AlbertII laan          27   \n",
      "2    1000              Brussel     de Berlaimontlaan          14   \n",
      "3    9100         Sint-Niklaas             Lamstraat         113   \n",
      "4    1210  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                        SearchQuery\n",
      "0                                                                     IGL 3600 Genk\n",
      "1                                                          PROXIMUS 1030 Schaarbeek\n",
      "2                                            Nationale Bank van België 1000 Brussel\n",
      "3    Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas\n",
      "4  Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"{row['OfficialName']} {row['ZipCode']} {row['Municipality']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org'\n",
    "                , 'trendstop.levif.be', 'companyweb.be', 'linkedin.com'\n",
    "                , 'en.wikipedia.org', 'facebook.com', 'be.linkedin.com'\n",
    "                , 'instagram.com', 'werkenbijdeoverheid.be', 'dnb.com', 'nl.wikipedia.org'\n",
    "                , 'youtube.com', 'staatsbladmonitor.be', 'werkenvoor.be'\n",
    "                , 'twitter.com', 'vlaanderen.be/organisaties', 'jobat.be'\n",
    "                , 'vdab.be']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    delay_between_requests = 0.1\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}\"\n",
    "        response = requests.get(url, params=kwargs)\n",
    "        response.raise_for_status()  # Check for HTTP-level issues\n",
    "        data = response.json()  # Parse JSON response\n",
    "        return data\n",
    "    except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "        print(f\"Encountered a network error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, min_results=5, max_retries=1):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) == min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += 10  # Prepare to query next page of results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(0.1)  # Short delay before retrying\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = 'search_results_Google.csv'\n",
    "    placeholder = \"\"  # Placeholder for unfound URLs\n",
    "    batch_size = 10  # Number of queries to process before saving to CSV\n",
    "    processed_batch = []  # Initialize the batch list\n",
    "\n",
    "    try:\n",
    "        progress_df = pd.read_csv(file_path)\n",
    "        last_processed_entity = progress_df['EntityNumber'].max() if not progress_df.empty else None\n",
    "    except FileNotFoundError:\n",
    "        progress_df = pd.DataFrame(columns=['EntityNumber', 'URL1', 'URL2', 'URL3', 'URL4', 'URL5'])\n",
    "        last_processed_entity = None\n",
    "\n",
    "    # Assuming 'df' is your DataFrame with the search queries\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        entity_number = row['EntityNumber']\n",
    "        if last_processed_entity and entity_number <= last_processed_entity:\n",
    "            continue\n",
    "\n",
    "        # Perform search and process results\n",
    "        filtered_urls = scrape_top_urls_google(row['SearchQuery'], skip_domains, 5)\n",
    "        urls_to_add = [filtered_urls[i] if i < len(filtered_urls) else placeholder for i in range(5)]\n",
    "        \n",
    "        processed_batch.append({\"EntityNumber\": entity_number, \"URL1\": urls_to_add[0], \"URL2\": urls_to_add[1], \"URL3\": urls_to_add[2], \"URL4\": urls_to_add[3], \"URL5\": urls_to_add[4]})\n",
    "\n",
    "        if len(processed_batch) >= batch_size or index == total_queries - 1:\n",
    "            # Append batch to DataFrame and reset for next batch\n",
    "            new_rows_df = pd.DataFrame(processed_batch)\n",
    "            progress_df = pd.concat([progress_df, new_rows_df], ignore_index=True)\n",
    "            progress_df.to_csv(file_path, index=False)\n",
    "            processed_batch = []  # Clear the batch\n",
    "\n",
    "        # Update progress\n",
    "        processed_entries = index + 1  # Assuming 'df' is zero-indexed\n",
    "        percentage_completed = (processed_entries / total_queries) * 100\n",
    "        print(f\"Processed EntityNumber {entity_number}. Completion: {percentage_completed:.2f}% [{processed_entries}/{total_queries}]\")\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "# Uncomment and call main() when ready\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\curl_cffi\\aio.py:39: RuntimeWarning: \n",
      "    Proactor event loop does not implement add_reader family of methods required.\n",
      "    Registering an additional selector thread for add_reader support.\n",
      "    To avoid this warning use:\n",
      "        asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n",
      "    \n",
      "  warnings.warn(PROACTOR_WARNING, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered an error: _extract_vqd() keywords='S!MPLR 2800 Mechelen' Could not extract vqd.\n",
      "Error encountered: _extract_vqd() keywords='S!MPLR 2800 Mechelen' Could not extract vqd.. Waiting before retrying...\n",
      "All data has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions. \n",
    "#%pip install -U duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "try:\n",
    "    result_df = pd.read_csv('search_results_DDG.csv')\n",
    "    collected_data = result_df.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    collected_data = []\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "        continue  # Skip already processed\n",
    "    \n",
    "    search_query = row['SearchQuery']\n",
    "    entity_number = row['EntityNumber']\n",
    "    try:\n",
    "        filtered_urls = scrape_top_urls_ddg(search_query, skip_domains, max_results=5)\n",
    "        time.sleep(1)  # Enforce a simple rate limit\n",
    "\n",
    "        collected_data.append({\n",
    "            \"EntityNumber\": entity_number, \n",
    "            \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\", \n",
    "            \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\", \n",
    "            \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\", \n",
    "            \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\", \n",
    "            \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Waiting before retrying...\")\n",
    "        time.sleep(20)  # Optional: Adjust based on your preference\n",
    "        continue  # Optionally retry the current iteration\n",
    "\n",
    "    result_df = pd.DataFrame(collected_data)\n",
    "    result_df.to_csv('search_results_DDG.csv', index=False)\n",
    "\n",
    "    processed_entries = index + 1  # Assuming 'df' is zero-indexed\n",
    "    percentage_completed = (processed_entries / total_rows) * 100\n",
    "    print(f\"Processed EntityNumber {entity_number}. Completion: {percentage_completed:.2f}% [{processed_entries}/{total_rows}]\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_11224\\1971050754.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  scores_df = pd.concat([scores_df, new_score_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entity number: 0550.830.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\curl_cffi\\aio.py:205: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entity number: 0794.745.447\n",
      "Processed entity number: 1002.072.554\n",
      "Processed entity number: 0846.276.302\n",
      "Processed entity number: 0843.041.846\n",
      "Processed entity number: 0521.815.062\n",
      "Processed entity number: 0790.679.167\n",
      "Processed entity number: 0655.858.075\n",
      "Processed entity number: 0844.223.959\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0788.423.522\n",
      "Processed entity number: 0677.672.187\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0787.341.179\n",
      "Processed entity number: 0771.345.087\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 1006.547.224\n",
      "Processed entity number: 0431.496.580\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0694.716.968\n",
      "Processed entity number: 0833.817.443\n",
      "Processed entity number: 0862.050.480\n",
      "Processed entity number: 0688.745.332\n",
      "Processed entity number: 0834.573.449\n",
      "Processed entity number: 0788.306.330\n",
      "Processed entity number: 0650.932.554\n",
      "Processed entity number: 0787.956.140\n",
      "Processed entity number: 0803.139.808\n",
      "Processed entity number: 0769.717.764\n",
      "Processed entity number: 0847.196.020\n",
      "Processed entity number: 0804.524.532\n",
      "Processed entity number: 0541.967.308\n",
      "Processed entity number: 1004.024.432\n",
      "Processed entity number: 0719.311.418\n",
      "Processed entity number: 0863.335.236\n",
      "Processed entity number: 0690.902.591\n",
      "Processed entity number: 0726.683.220\n",
      "Processed entity number: 0841.693.447\n",
      "Processed entity number: 0841.029.986\n",
      "Processed entity number: 0542.560.689\n",
      "Processed entity number: 0479.953.228\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0649.780.630\n",
      "Processed entity number: 0723.542.596\n",
      "Processed entity number: 0835.089.331\n",
      "Processed entity number: 0840.843.807\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0797.377.216\n",
      "Processed entity number: 0704.740.335\n",
      "Processed entity number: 0524.966.671\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0805.873.822\n",
      "Processed entity number: 0811.114.196\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0874.879.919\n",
      "Processed entity number: 0540.774.406\n",
      "Processed entity number: 0474.561.414\n",
      "Processed entity number: 0849.220.449\n",
      "Processed entity number: 0715.682.925\n",
      "Processed entity number: 0643.901.143\n",
      "Processed entity number: 1001.840.447\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0797.537.364\n",
      "Processed entity number: 0673.646.390\n",
      "Processed entity number: 0643.657.257\n",
      "Processed entity number: 0689.691.279\n",
      "Processed entity number: 0808.357.319\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 1001.742.259\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0503.879.465\n",
      "Processed entity number: 0552.795.575\n",
      "Processed entity number: 0502.583.031\n",
      "Processed entity number: 0473.638.330\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0793.284.608\n",
      "Processed entity number: 0793.810.980\n",
      "Processed entity number: 0643.989.334\n",
      "Processed entity number: 0792.781.097\n",
      "Processed entity number: 0833.083.411\n",
      "Processed entity number: 1006.399.051\n",
      "Processed entity number: 0472.281.815\n",
      "Processed entity number: 0806.979.226\n",
      "Processed entity number: 0818.981.391\n",
      "Processed entity number: 0802.870.582\n",
      "Processed entity number: 0871.602.210\n",
      "Processed entity number: 0792.946.492\n",
      "Processed entity number: 0472.716.137\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0656.775.617\n",
      "Processed entity number: 0797.152.532\n",
      "Processed entity number: 0647.824.594\n",
      "Processed entity number: 0766.647.220\n",
      "Processed entity number: 0797.424.132\n",
      "Processed entity number: 0789.228.523\n",
      "Processed entity number: 0535.665.474\n",
      "Processed entity number: 0698.559.653\n",
      "No results found in attempt 1. Retrying...\n",
      "No results found in attempt 1. Retrying...\n",
      "Processed entity number: 0831.755.402\n",
      "Processed entity number: 0692.761.033\n",
      "Processed entity number: 0473.064.248\n",
      "Processed entity number: 0837.119.403\n",
      "Processed entity number: 0680.877.840\n",
      "Processed entity number: 0792.816.137\n",
      "Processed entity number: 0797.507.472\n",
      "Processed entity number: 0550.572.295\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from duckduckgo_search import DDGS\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "# Rate limiting delay\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Adjust as per your rate limit settings\n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    delay_between_requests = DELAY_BETWEEN_REQUESTS\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}\"\n",
    "        response = requests.get(url, params=kwargs)\n",
    "        response.raise_for_status()  # Check for HTTP-level issues\n",
    "        data = response.json()  # Parse JSON response\n",
    "        return data\n",
    "    except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "        print(f\"Encountered a network error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, min_results=5, max_retries=1):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) == min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += 10  # Prepare to query next page of results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)  # Short delay before retrying\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    google_results = scrape_top_urls_google(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    # New logic to calculate scores\n",
    "    combined_scores = calculate_combined_rank_score(google_results, ddg_results)\n",
    "\n",
    "    # Select top URLs based on the new scoring logic\n",
    "    top_urls_with_scores = combined_scores[:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "def calculate_combined_rank_score(google_results, ddg_results):\n",
    "    combined_scores = {}\n",
    "    max_rank = max(len(google_results), len(ddg_results)) + 1\n",
    "\n",
    "    all_urls = set(google_results + ddg_results)\n",
    "    for url in all_urls:\n",
    "        google_rank = google_results.index(url) + 1 if url in google_results else max_rank\n",
    "        ddg_rank = ddg_results.index(url) + 1 if url in ddg_results else max_rank\n",
    "        combined_score = google_rank * ddg_rank\n",
    "        combined_scores[url] = combined_score\n",
    "\n",
    "    sorted_urls_with_scores = sorted(combined_scores.items(), key=lambda item: item[1])\n",
    "    final_scores = [(url, 1.0 / combined_score) for url, combined_score in sorted_urls_with_scores]\n",
    "    return final_scores\n",
    "\n",
    "\n",
    "def save_last_processed_entity(entity_number):\n",
    "    with open('last_processed_entity.txt', 'w') as file:\n",
    "        file.write(str(entity_number))\n",
    "\n",
    "def get_last_processed_entity():\n",
    "    try:\n",
    "        with open('last_processed_entity.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def find_missing_entities(df, csv_path='search_results_Multi.csv'):\n",
    "    \"\"\"Find entity numbers in df that are missing in the CSV.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return set(df['EntityNumber'])  # If CSV doesn't exist, all entities are missing\n",
    "\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    existing_entities = set(existing_df['EntityNumber'])\n",
    "    all_entities = set(df['EntityNumber'])\n",
    "    \n",
    "    missing_entities = all_entities - existing_entities\n",
    "    return missing_entities\n",
    "\n",
    "def process_entity(entity_number, query, skip_domains):\n",
    "    \"\"\"Process a single entity number.\"\"\"\n",
    "    top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5)\n",
    "    top_urls = [url for url, score in top_urls_with_scores]\n",
    "\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"EntityNumber\": entity_number, \n",
    "        \"URL1\": top_urls[0] if len(top_urls) > 0 else \"\", \n",
    "        \"URL2\": top_urls[1] if len(top_urls) > 1 else \"\", \n",
    "        \"URL3\": top_urls[2] if len(top_urls) > 2 else \"\", \n",
    "        \"URL4\": top_urls[3] if len(top_urls) > 3 else \"\", \n",
    "        \"URL5\": top_urls[4] if len(top_urls) > 4 else \"\"\n",
    "    }])\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "def append_to_csv(new_row, file_name='search_results_Multi.csv'):\n",
    "    with open(file_name, 'a', newline='') as file:\n",
    "        new_row.to_csv(file, header=file.tell()==0, index=False)\n",
    "        \n",
    "def main():\n",
    "    scores_df = pd.DataFrame(columns=['EntityNumber', 'URL', 'Score'])\n",
    "    # Assume df is defined or loaded earlier in the script.\n",
    "    missing_entities = find_missing_entities(df)\n",
    "    \n",
    "    for entity_number in missing_entities:\n",
    "        row = df[df['EntityNumber'] == entity_number].iloc[0]  # Assuming entity number is unique\n",
    "        query = row['SearchQuery']\n",
    "        \n",
    "        # Process each entity\n",
    "        new_row = process_entity(entity_number, query, skip_domains)\n",
    "        append_to_csv(new_row)\n",
    "        save_last_processed_entity(entity_number)\n",
    "        \n",
    "        # Perform search to get URLs and scores\n",
    "        top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5)\n",
    "        \n",
    "        # After collecting URLs and scores, append them to the scores data frame\n",
    "        for url, score in top_urls_with_scores:\n",
    "            new_score_row = pd.DataFrame({'EntityNumber': [entity_number], 'URL': [url], 'Score': [score]})\n",
    "            scores_df = pd.concat([scores_df, new_score_row], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed entity number: {entity_number}\")\n",
    "    \n",
    "    # Optionally save scores_df to a file or handle it as needed\n",
    "    scores_df.to_csv('final_scores.csv', index=False)\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if all entity number are present and rearagne the entities in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entity_numbers_in_csv(df, csv_path):\n",
    "    \"\"\"\n",
    "    Check if all entity numbers in the dataframe (df) are present in the CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing entity numbers.\n",
    "    - csv_path: The path to the CSV file to check against.\n",
    "    \n",
    "    Returns:\n",
    "    - (bool, list): A tuple containing a boolean indicating if all entities are present, and a list of missing entities if any.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return False, list(df['EntityNumber'])  # If CSV doesn't exist, all entities are missing\n",
    "    \n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "    missing_entities = set(df['EntityNumber']) - set(csv_df['EntityNumber'])\n",
    "    \n",
    "    return len(missing_entities) == 0, list(missing_entities)\n",
    "\n",
    "def reorder_csv_to_match_df(df, csv_path):\n",
    "    \"\"\"\n",
    "    Reorder the entries in the CSV file to match the order of entries in the DataFrame (df) and overwrite the original CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the desired order of entity numbers.\n",
    "    - csv_path: The path to the CSV file to reorder and overwrite.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV file does not exist.\")\n",
    "        return\n",
    "    \n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Reorder the csv_df to match the order in df\n",
    "    reordered_df = pd.merge(df[['EntityNumber']], csv_df, on='EntityNumber', how='left')\n",
    "    \n",
    "    # Overwrite the original CSV file with the reordered DataFrame\n",
    "    reordered_df.to_csv(csv_path, index=False)\n",
    "    print(f\"CSV has been reordered and saved to {csv_path}.\")\n",
    "\n",
    "# Use of functions\n",
    "all_present, missing_entities = check_entity_numbers_in_csv(df, 'path/to/your_csv.csv')\n",
    "if all_present:\n",
    "    print(\"All entity numbers are present in the CSV.\")\n",
    "else:\n",
    "    print(f\"Missing entity numbers: {missing_entities}\")\n",
    "\n",
    "reorder_csv_to_match_df(df, 'path/to/your_csv.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
