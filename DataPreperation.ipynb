{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: lxml>=5.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from click>=8.1.7->duckduckgo_search->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                       OfficialName ZipCode  \\\n",
      "0  0201.310.929                                                IGL    3600   \n",
      "1  0202.239.951                                           PROXIMUS    1030   \n",
      "2  0203.201.340                          Nationale Bank van BelgiÃ«    1000   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...    9100   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...    1210   \n",
      "\n",
      "          Municipality                Street HouseNumber  \\\n",
      "0                 Genk            Klotstraat         125   \n",
      "1           Schaarbeek  Koning AlbertII laan          27   \n",
      "2              Brussel     de Berlaimontlaan          14   \n",
      "3         Sint-Niklaas             Lamstraat         113   \n",
      "4  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              SearchQuery\n",
      "0                                                                    Website IGL 3600 Genk Klotstraat 125\n",
      "1                                                Website PROXIMUS 1030 Schaarbeek Koning AlbertII laan 27\n",
      "2                                     Website Nationale Bank van BelgiÃ« 1000 Brussel de Berlaimontlaan 14\n",
      "3    Website Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas Lamstraat 113\n",
      "4  Website Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node Galileelaan 5\n",
      "Website DATACTION 9320 Aalst Ninovesteenweg 198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"Website {row['OfficialName']} {row['ZipCode']} {row['Municipality']} {row['Street']} {row['HouseNumber']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())\n",
    "print(df.iloc[1600]['SearchQuery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org', 'trendstop.levif.be', 'www.companyweb.be', 'www.linkedin.com', 'https://www.companyweb.be', 'https://bizzy.org', 'https://www.linkedin.com', 'https://bizzy.org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 / 17239. Queries made: 1\n",
      "Processed 2 / 17239. Queries made: 2\n",
      "Processed 3 / 17239. Queries made: 3\n",
      "Processed 4 / 17239. Queries made: 4\n",
      "Processed 5 / 17239. Queries made: 5\n",
      "Processed 6 / 17239. Queries made: 6\n",
      "Processed 7 / 17239. Queries made: 7\n",
      "Processed 8 / 17239. Queries made: 8\n",
      "Processed 9 / 17239. Queries made: 9\n",
      "Processed 10 / 17239. Queries made: 10\n",
      "Processed 11 / 17239. Queries made: 11\n",
      "Processed 12 / 17239. Queries made: 12\n",
      "Processed 14 / 17239. Queries made: 13\n",
      "Processed 15 / 17239. Queries made: 14\n",
      "Processed 16 / 17239. Queries made: 15\n",
      "Processed 17 / 17239. Queries made: 16\n",
      "Processed 18 / 17239. Queries made: 17\n",
      "Processed 19 / 17239. Queries made: 18\n",
      "Processed 20 / 17239. Queries made: 19\n",
      "Processed 21 / 17239. Queries made: 20\n",
      "Processed 22 / 17239. Queries made: 21\n",
      "Processed 23 / 17239. Queries made: 22\n",
      "Processed 24 / 17239. Queries made: 23\n",
      "Processed 25 / 17239. Queries made: 24\n",
      "Processed 26 / 17239. Queries made: 25\n",
      "Processed 27 / 17239. Queries made: 26\n",
      "Processed 28 / 17239. Queries made: 27\n",
      "Processed 29 / 17239. Queries made: 28\n",
      "Processed 30 / 17239. Queries made: 29\n",
      "Processed 31 / 17239. Queries made: 30\n",
      "Processed 32 / 17239. Queries made: 31\n",
      "Processed 33 / 17239. Queries made: 32\n",
      "Processed 34 / 17239. Queries made: 33\n",
      "Processed 35 / 17239. Queries made: 34\n",
      "Processed 36 / 17239. Queries made: 35\n",
      "Processed 37 / 17239. Queries made: 36\n",
      "Processed 38 / 17239. Queries made: 37\n",
      "Processed 39 / 17239. Queries made: 38\n",
      "Processed 40 / 17239. Queries made: 39\n",
      "Processed 41 / 17239. Queries made: 40\n",
      "Processed 42 / 17239. Queries made: 41\n",
      "Processed 43 / 17239. Queries made: 42\n",
      "Processed 44 / 17239. Queries made: 43\n",
      "Processed 45 / 17239. Queries made: 44\n",
      "Processed 46 / 17239. Queries made: 45\n",
      "Processed 47 / 17239. Queries made: 46\n",
      "Processed 48 / 17239. Queries made: 47\n",
      "Processed 49 / 17239. Queries made: 48\n",
      "Processed 50 / 17239. Queries made: 49\n",
      "Processed 52 / 17239. Queries made: 50\n",
      "Processed 53 / 17239. Queries made: 51\n",
      "Processed 54 / 17239. Queries made: 52\n",
      "Processed 55 / 17239. Queries made: 53\n",
      "Processed 56 / 17239. Queries made: 54\n",
      "Processed 57 / 17239. Queries made: 55\n",
      "Processed 58 / 17239. Queries made: 56\n",
      "Processed 59 / 17239. Queries made: 57\n",
      "Processed 60 / 17239. Queries made: 58\n",
      "Processed 61 / 17239. Queries made: 59\n",
      "Processed 62 / 17239. Queries made: 60\n",
      "Processed 63 / 17239. Queries made: 61\n",
      "Processed 64 / 17239. Queries made: 62\n",
      "Processed 65 / 17239. Queries made: 63\n",
      "Processed 66 / 17239. Queries made: 64\n",
      "Processed 67 / 17239. Queries made: 65\n",
      "Processed 68 / 17239. Queries made: 66\n",
      "Processed 69 / 17239. Queries made: 67\n",
      "Processed 70 / 17239. Queries made: 68\n",
      "Processed 71 / 17239. Queries made: 69\n",
      "Processed 72 / 17239. Queries made: 70\n",
      "Processed 73 / 17239. Queries made: 71\n",
      "Processed 74 / 17239. Queries made: 72\n",
      "Processed 75 / 17239. Queries made: 73\n",
      "Processed 76 / 17239. Queries made: 74\n",
      "Processed 77 / 17239. Queries made: 75\n",
      "Processed 78 / 17239. Queries made: 76\n",
      "Processed 79 / 17239. Queries made: 77\n",
      "Processed 80 / 17239. Queries made: 78\n",
      "Processed 81 / 17239. Queries made: 79\n",
      "Processed 82 / 17239. Queries made: 80\n",
      "Processed 83 / 17239. Queries made: 81\n",
      "Processed 84 / 17239. Queries made: 82\n",
      "Processed 85 / 17239. Queries made: 83\n",
      "Processed 86 / 17239. Queries made: 84\n",
      "Processed 87 / 17239. Queries made: 85\n",
      "Processed 88 / 17239. Queries made: 86\n",
      "Processed 89 / 17239. Queries made: 87\n",
      "Processed 90 / 17239. Queries made: 88\n",
      "Processed 91 / 17239. Queries made: 89\n",
      "Processed 92 / 17239. Queries made: 90\n",
      "Processed 93 / 17239. Queries made: 91\n",
      "Processed 94 / 17239. Queries made: 92\n",
      "Processed 95 / 17239. Queries made: 93\n",
      "Processed 96 / 17239. Queries made: 94\n",
      "Processed 97 / 17239. Queries made: 95\n",
      "Processed 100 / 17239. Queries made: 96\n",
      "Processed 101 / 17239. Queries made: 97\n",
      "Processed 102 / 17239. Queries made: 98\n",
      "Processed 103 / 17239. Queries made: 99\n",
      "Processed 104 / 17239. Queries made: 100\n",
      "Processed 105 / 17239. Queries made: 101\n",
      "Processed 106 / 17239. Queries made: 102\n",
      "Processed 107 / 17239. Queries made: 103\n",
      "Processed 108 / 17239. Queries made: 104\n",
      "Processed 109 / 17239. Queries made: 105\n",
      "Processed 110 / 17239. Queries made: 106\n",
      "Processed 111 / 17239. Queries made: 107\n",
      "Processed 112 / 17239. Queries made: 108\n",
      "Processed 113 / 17239. Queries made: 109\n",
      "Processed 114 / 17239. Queries made: 110\n",
      "Processed 115 / 17239. Queries made: 111\n",
      "Processed 116 / 17239. Queries made: 112\n",
      "Processed 117 / 17239. Queries made: 113\n",
      "Processed 118 / 17239. Queries made: 114\n",
      "Processed 119 / 17239. Queries made: 115\n",
      "Processed 120 / 17239. Queries made: 116\n",
      "Processed 121 / 17239. Queries made: 117\n",
      "Processed 122 / 17239. Queries made: 118\n",
      "Processed 123 / 17239. Queries made: 119\n",
      "Processed 124 / 17239. Queries made: 120\n",
      "Processed 125 / 17239. Queries made: 121\n",
      "Processed 126 / 17239. Queries made: 122\n",
      "Processed 127 / 17239. Queries made: 123\n",
      "Processed 128 / 17239. Queries made: 124\n",
      "Processed 129 / 17239. Queries made: 125\n",
      "Processed 130 / 17239. Queries made: 126\n",
      "Processed 131 / 17239. Queries made: 127\n",
      "Processed 132 / 17239. Queries made: 128\n",
      "Processed 133 / 17239. Queries made: 129\n",
      "Processed 134 / 17239. Queries made: 130\n",
      "Processed 135 / 17239. Queries made: 131\n",
      "Processed 136 / 17239. Queries made: 132\n",
      "Processed 137 / 17239. Queries made: 133\n",
      "Processed 138 / 17239. Queries made: 134\n",
      "Processed 139 / 17239. Queries made: 135\n",
      "Processed 140 / 17239. Queries made: 136\n",
      "Processed 141 / 17239. Queries made: 137\n",
      "Processed 142 / 17239. Queries made: 138\n",
      "Processed 143 / 17239. Queries made: 139\n",
      "Processed 144 / 17239. Queries made: 140\n",
      "Processed 145 / 17239. Queries made: 141\n",
      "Processed 146 / 17239. Queries made: 142\n",
      "Processed 147 / 17239. Queries made: 143\n",
      "Processed 148 / 17239. Queries made: 144\n",
      "Processed 149 / 17239. Queries made: 145\n",
      "Processed 150 / 17239. Queries made: 146\n",
      "Processed 151 / 17239. Queries made: 147\n",
      "Processed 155 / 17239. Queries made: 148\n",
      "Processed 156 / 17239. Queries made: 149\n",
      "Processed 157 / 17239. Queries made: 150\n",
      "Processed 158 / 17239. Queries made: 151\n",
      "Processed 159 / 17239. Queries made: 152\n",
      "Processed 160 / 17239. Queries made: 153\n",
      "Processed 161 / 17239. Queries made: 154\n",
      "Processed 162 / 17239. Queries made: 155\n",
      "Processed 163 / 17239. Queries made: 156\n",
      "Processed 164 / 17239. Queries made: 157\n",
      "Processed 165 / 17239. Queries made: 158\n",
      "Processed 166 / 17239. Queries made: 159\n",
      "Processed 167 / 17239. Queries made: 160\n",
      "Processed 168 / 17239. Queries made: 161\n",
      "Processed 169 / 17239. Queries made: 162\n",
      "Processed 170 / 17239. Queries made: 163\n",
      "Processed 171 / 17239. Queries made: 164\n",
      "Processed 172 / 17239. Queries made: 165\n",
      "Processed 173 / 17239. Queries made: 166\n",
      "Processed 174 / 17239. Queries made: 167\n",
      "Processed 175 / 17239. Queries made: 168\n",
      "Processed 176 / 17239. Queries made: 169\n",
      "Processed 177 / 17239. Queries made: 170\n",
      "Processed 178 / 17239. Queries made: 171\n",
      "Processed 179 / 17239. Queries made: 172\n",
      "Processed 180 / 17239. Queries made: 173\n",
      "Processed 181 / 17239. Queries made: 174\n",
      "Processed 182 / 17239. Queries made: 175\n",
      "Processed 183 / 17239. Queries made: 176\n",
      "Processed 184 / 17239. Queries made: 177\n",
      "Processed 185 / 17239. Queries made: 178\n",
      "Processed 186 / 17239. Queries made: 179\n",
      "Processed 187 / 17239. Queries made: 180\n",
      "Processed 188 / 17239. Queries made: 181\n",
      "Processed 189 / 17239. Queries made: 182\n",
      "Processed 190 / 17239. Queries made: 183\n",
      "Processed 191 / 17239. Queries made: 184\n",
      "Processed 192 / 17239. Queries made: 185\n",
      "Processed 193 / 17239. Queries made: 186\n",
      "Processed 197 / 17239. Queries made: 187\n",
      "Processed 198 / 17239. Queries made: 188\n",
      "Processed 199 / 17239. Queries made: 189\n",
      "Processed 200 / 17239. Queries made: 190\n",
      "Processed 201 / 17239. Queries made: 191\n",
      "Processed 202 / 17239. Queries made: 192\n",
      "Processed 203 / 17239. Queries made: 193\n",
      "Processed 204 / 17239. Queries made: 194\n",
      "Processed 205 / 17239. Queries made: 195\n",
      "Processed 206 / 17239. Queries made: 196\n",
      "Processed 207 / 17239. Queries made: 197\n",
      "Processed 208 / 17239. Queries made: 198\n",
      "Processed 209 / 17239. Queries made: 199\n",
      "Processed 210 / 17239. Queries made: 200\n",
      "Processed 211 / 17239. Queries made: 201\n",
      "Processed 212 / 17239. Queries made: 202\n",
      "Processed 213 / 17239. Queries made: 203\n",
      "Processed 214 / 17239. Queries made: 204\n",
      "Processed 215 / 17239. Queries made: 205\n",
      "Processed 216 / 17239. Queries made: 206\n",
      "Processed 217 / 17239. Queries made: 207\n",
      "Processed 218 / 17239. Queries made: 208\n",
      "Processed 219 / 17239. Queries made: 209\n",
      "Processed 220 / 17239. Queries made: 210\n",
      "Processed 221 / 17239. Queries made: 211\n",
      "Processed 222 / 17239. Queries made: 212\n",
      "Processed 223 / 17239. Queries made: 213\n",
      "Processed 224 / 17239. Queries made: 214\n",
      "Processed 225 / 17239. Queries made: 215\n",
      "Processed 227 / 17239. Queries made: 216\n",
      "Processed 228 / 17239. Queries made: 217\n",
      "Processed 229 / 17239. Queries made: 218\n",
      "Processed 230 / 17239. Queries made: 219\n",
      "Processed 231 / 17239. Queries made: 220\n",
      "Processed 232 / 17239. Queries made: 221\n",
      "Processed 233 / 17239. Queries made: 222\n",
      "Processed 235 / 17239. Queries made: 223\n",
      "Processed 236 / 17239. Queries made: 224\n",
      "Processed 237 / 17239. Queries made: 225\n",
      "Processed 238 / 17239. Queries made: 226\n",
      "Processed 239 / 17239. Queries made: 227\n",
      "Processed 240 / 17239. Queries made: 228\n",
      "Processed 241 / 17239. Queries made: 229\n",
      "Processed 242 / 17239. Queries made: 230\n",
      "Processed 243 / 17239. Queries made: 231\n",
      "Processed 244 / 17239. Queries made: 232\n",
      "Processed 248 / 17239. Queries made: 233\n",
      "Processed 249 / 17239. Queries made: 234\n",
      "Processed 250 / 17239. Queries made: 235\n",
      "Processed 251 / 17239. Queries made: 236\n",
      "Processed 252 / 17239. Queries made: 237\n",
      "Processed 253 / 17239. Queries made: 238\n",
      "Processed 254 / 17239. Queries made: 239\n",
      "Processed 255 / 17239. Queries made: 240\n",
      "Processed 256 / 17239. Queries made: 241\n",
      "Processed 257 / 17239. Queries made: 242\n",
      "Processed 258 / 17239. Queries made: 243\n",
      "Processed 259 / 17239. Queries made: 244\n",
      "Processed 260 / 17239. Queries made: 245\n",
      "Processed 261 / 17239. Queries made: 246\n",
      "Processed 262 / 17239. Queries made: 247\n",
      "Processed 263 / 17239. Queries made: 248\n",
      "Processed 264 / 17239. Queries made: 249\n",
      "Processed 265 / 17239. Queries made: 250\n",
      "Processed 266 / 17239. Queries made: 251\n",
      "Processed 267 / 17239. Queries made: 252\n",
      "Processed 268 / 17239. Queries made: 253\n",
      "Processed 269 / 17239. Queries made: 254\n",
      "Processed 270 / 17239. Queries made: 255\n",
      "Processed 271 / 17239. Queries made: 256\n",
      "Processed 272 / 17239. Queries made: 257\n",
      "Processed 273 / 17239. Queries made: 258\n",
      "Processed 274 / 17239. Queries made: 259\n",
      "Processed 275 / 17239. Queries made: 260\n",
      "Processed 277 / 17239. Queries made: 261\n",
      "Processed 278 / 17239. Queries made: 262\n",
      "Processed 279 / 17239. Queries made: 263\n",
      "Processed 280 / 17239. Queries made: 264\n",
      "Processed 281 / 17239. Queries made: 265\n",
      "Processed 282 / 17239. Queries made: 266\n",
      "Processed 283 / 17239. Queries made: 267\n",
      "Processed 284 / 17239. Queries made: 268\n",
      "Processed 285 / 17239. Queries made: 269\n",
      "Processed 286 / 17239. Queries made: 270\n",
      "Processed 287 / 17239. Queries made: 271\n",
      "Processed 288 / 17239. Queries made: 272\n",
      "Processed 289 / 17239. Queries made: 273\n",
      "Processed 290 / 17239. Queries made: 274\n",
      "Processed 291 / 17239. Queries made: 275\n",
      "Processed 292 / 17239. Queries made: 276\n",
      "Processed 293 / 17239. Queries made: 277\n",
      "Processed 294 / 17239. Queries made: 278\n",
      "Processed 295 / 17239. Queries made: 279\n",
      "Processed 296 / 17239. Queries made: 280\n",
      "Processed 297 / 17239. Queries made: 281\n",
      "Processed 298 / 17239. Queries made: 282\n",
      "Processed 299 / 17239. Queries made: 283\n",
      "Processed 300 / 17239. Queries made: 284\n",
      "Processed 301 / 17239. Queries made: 285\n",
      "Processed 302 / 17239. Queries made: 286\n",
      "Processed 303 / 17239. Queries made: 287\n",
      "Processed 304 / 17239. Queries made: 288\n",
      "Processed 305 / 17239. Queries made: 289\n",
      "Processed 306 / 17239. Queries made: 290\n",
      "Processed 307 / 17239. Queries made: 291\n",
      "Processed 308 / 17239. Queries made: 292\n",
      "Processed 309 / 17239. Queries made: 293\n",
      "Processed 310 / 17239. Queries made: 294\n",
      "Processed 311 / 17239. Queries made: 295\n",
      "Processed 313 / 17239. Queries made: 296\n",
      "Processed 314 / 17239. Queries made: 297\n",
      "Processed 315 / 17239. Queries made: 298\n",
      "Processed 316 / 17239. Queries made: 299\n",
      "Processed 317 / 17239. Queries made: 300\n",
      "Processed 318 / 17239. Queries made: 301\n",
      "Processed 319 / 17239. Queries made: 302\n",
      "Processed 320 / 17239. Queries made: 303\n",
      "Processed 321 / 17239. Queries made: 304\n",
      "Processed 322 / 17239. Queries made: 305\n",
      "Processed 323 / 17239. Queries made: 306\n",
      "Processed 324 / 17239. Queries made: 307\n",
      "Processed 325 / 17239. Queries made: 308\n",
      "Processed 326 / 17239. Queries made: 309\n",
      "Processed 327 / 17239. Queries made: 310\n",
      "Processed 328 / 17239. Queries made: 311\n",
      "Processed 329 / 17239. Queries made: 312\n",
      "Processed 330 / 17239. Queries made: 313\n",
      "Processed 331 / 17239. Queries made: 314\n",
      "Processed 332 / 17239. Queries made: 315\n",
      "Processed 333 / 17239. Queries made: 316\n",
      "Processed 334 / 17239. Queries made: 317\n",
      "Processed 335 / 17239. Queries made: 318\n",
      "Processed 336 / 17239. Queries made: 319\n",
      "Processed 337 / 17239. Queries made: 320\n",
      "Processed 338 / 17239. Queries made: 321\n",
      "Processed 340 / 17239. Queries made: 322\n",
      "Processed 341 / 17239. Queries made: 323\n",
      "Processed 342 / 17239. Queries made: 324\n",
      "Processed 345 / 17239. Queries made: 325\n",
      "Processed 346 / 17239. Queries made: 326\n",
      "Processed 347 / 17239. Queries made: 327\n",
      "Processed 348 / 17239. Queries made: 328\n",
      "Processed 349 / 17239. Queries made: 329\n",
      "Processed 350 / 17239. Queries made: 330\n",
      "Processed 351 / 17239. Queries made: 331\n",
      "Processed 352 / 17239. Queries made: 332\n",
      "Processed 353 / 17239. Queries made: 333\n",
      "Processed 354 / 17239. Queries made: 334\n",
      "Processed 355 / 17239. Queries made: 335\n",
      "Processed 356 / 17239. Queries made: 336\n",
      "Processed 357 / 17239. Queries made: 337\n",
      "Processed 358 / 17239. Queries made: 338\n",
      "Processed 359 / 17239. Queries made: 339\n",
      "Processed 360 / 17239. Queries made: 340\n",
      "Processed 361 / 17239. Queries made: 341\n",
      "Processed 362 / 17239. Queries made: 342\n",
      "Processed 363 / 17239. Queries made: 343\n",
      "Processed 364 / 17239. Queries made: 344\n",
      "Processed 365 / 17239. Queries made: 345\n",
      "Processed 366 / 17239. Queries made: 346\n",
      "Processed 367 / 17239. Queries made: 347\n",
      "Processed 368 / 17239. Queries made: 348\n",
      "Processed 369 / 17239. Queries made: 349\n",
      "Processed 370 / 17239. Queries made: 350\n",
      "Processed 371 / 17239. Queries made: 351\n",
      "Processed 372 / 17239. Queries made: 352\n",
      "Processed 373 / 17239. Queries made: 353\n",
      "Processed 374 / 17239. Queries made: 354\n",
      "Processed 375 / 17239. Queries made: 355\n",
      "Processed 376 / 17239. Queries made: 356\n",
      "Processed 377 / 17239. Queries made: 357\n",
      "Processed 378 / 17239. Queries made: 358\n",
      "Processed 379 / 17239. Queries made: 359\n",
      "Processed 380 / 17239. Queries made: 360\n",
      "Processed 381 / 17239. Queries made: 361\n",
      "Processed 382 / 17239. Queries made: 362\n",
      "Processed 383 / 17239. Queries made: 363\n",
      "Processed 384 / 17239. Queries made: 364\n",
      "Processed 385 / 17239. Queries made: 365\n",
      "Processed 386 / 17239. Queries made: 366\n",
      "Processed 387 / 17239. Queries made: 367\n",
      "Processed 388 / 17239. Queries made: 368\n",
      "Processed 389 / 17239. Queries made: 369\n",
      "Processed 390 / 17239. Queries made: 370\n",
      "Processed 391 / 17239. Queries made: 371\n",
      "Processed 392 / 17239. Queries made: 372\n",
      "Processed 393 / 17239. Queries made: 373\n",
      "Processed 394 / 17239. Queries made: 374\n",
      "Processed 395 / 17239. Queries made: 375\n",
      "Processed 396 / 17239. Queries made: 376\n",
      "Processed 397 / 17239. Queries made: 377\n",
      "Processed 398 / 17239. Queries made: 378\n",
      "Processed 399 / 17239. Queries made: 379\n",
      "Processed 400 / 17239. Queries made: 380\n",
      "Processed 401 / 17239. Queries made: 381\n",
      "Processed 402 / 17239. Queries made: 382\n",
      "Processed 403 / 17239. Queries made: 383\n",
      "Processed 404 / 17239. Queries made: 384\n",
      "Processed 405 / 17239. Queries made: 385\n",
      "Processed 406 / 17239. Queries made: 386\n",
      "Processed 407 / 17239. Queries made: 387\n",
      "Processed 409 / 17239. Queries made: 388\n",
      "Processed 410 / 17239. Queries made: 389\n",
      "Processed 411 / 17239. Queries made: 390\n",
      "Processed 412 / 17239. Queries made: 391\n",
      "Processed 413 / 17239. Queries made: 392\n",
      "Processed 414 / 17239. Queries made: 393\n",
      "Processed 415 / 17239. Queries made: 394\n",
      "Processed 416 / 17239. Queries made: 395\n",
      "Processed 417 / 17239. Queries made: 396\n",
      "Processed 418 / 17239. Queries made: 397\n",
      "Processed 419 / 17239. Queries made: 398\n",
      "Processed 420 / 17239. Queries made: 399\n",
      "Processed 421 / 17239. Queries made: 400\n",
      "Processed 423 / 17239. Queries made: 401\n",
      "Processed 424 / 17239. Queries made: 402\n",
      "Processed 425 / 17239. Queries made: 403\n",
      "Processed 426 / 17239. Queries made: 404\n",
      "Processed 427 / 17239. Queries made: 405\n",
      "Processed 428 / 17239. Queries made: 406\n",
      "Processed 429 / 17239. Queries made: 407\n",
      "Processed 430 / 17239. Queries made: 408\n",
      "Processed 431 / 17239. Queries made: 409\n",
      "Processed 432 / 17239. Queries made: 410\n",
      "Processed 433 / 17239. Queries made: 411\n",
      "Processed 434 / 17239. Queries made: 412\n",
      "Processed 435 / 17239. Queries made: 413\n",
      "Processed 436 / 17239. Queries made: 414\n",
      "Processed 437 / 17239. Queries made: 415\n",
      "Processed 438 / 17239. Queries made: 416\n",
      "Processed 439 / 17239. Queries made: 417\n",
      "Processed 440 / 17239. Queries made: 418\n",
      "Processed 441 / 17239. Queries made: 419\n",
      "Processed 442 / 17239. Queries made: 420\n",
      "Processed 444 / 17239. Queries made: 421\n",
      "Processed 445 / 17239. Queries made: 422\n",
      "Processed 446 / 17239. Queries made: 423\n",
      "Processed 447 / 17239. Queries made: 424\n",
      "Processed 448 / 17239. Queries made: 425\n",
      "Processed 449 / 17239. Queries made: 426\n",
      "Processed 450 / 17239. Queries made: 427\n",
      "Processed 451 / 17239. Queries made: 428\n",
      "Processed 452 / 17239. Queries made: 429\n",
      "Processed 453 / 17239. Queries made: 430\n",
      "Processed 454 / 17239. Queries made: 431\n",
      "Processed 455 / 17239. Queries made: 432\n",
      "Processed 456 / 17239. Queries made: 433\n",
      "Processed 457 / 17239. Queries made: 434\n",
      "Processed 458 / 17239. Queries made: 435\n",
      "Processed 460 / 17239. Queries made: 436\n",
      "Processed 461 / 17239. Queries made: 437\n",
      "Processed 462 / 17239. Queries made: 438\n",
      "Processed 463 / 17239. Queries made: 439\n",
      "Processed 464 / 17239. Queries made: 440\n",
      "Processed 465 / 17239. Queries made: 441\n",
      "Processed 466 / 17239. Queries made: 442\n",
      "Processed 467 / 17239. Queries made: 443\n",
      "Processed 468 / 17239. Queries made: 444\n",
      "Processed 469 / 17239. Queries made: 445\n",
      "Processed 470 / 17239. Queries made: 446\n",
      "Processed 471 / 17239. Queries made: 447\n",
      "Processed 472 / 17239. Queries made: 448\n",
      "Processed 473 / 17239. Queries made: 449\n",
      "Processed 474 / 17239. Queries made: 450\n",
      "Processed 475 / 17239. Queries made: 451\n",
      "Processed 476 / 17239. Queries made: 452\n",
      "Processed 477 / 17239. Queries made: 453\n",
      "Processed 478 / 17239. Queries made: 454\n",
      "Processed 479 / 17239. Queries made: 455\n",
      "Processed 480 / 17239. Queries made: 456\n",
      "Processed 481 / 17239. Queries made: 457\n",
      "Processed 482 / 17239. Queries made: 458\n",
      "Processed 483 / 17239. Queries made: 459\n",
      "Processed 484 / 17239. Queries made: 460\n",
      "Processed 485 / 17239. Queries made: 461\n",
      "Processed 486 / 17239. Queries made: 462\n",
      "Processed 487 / 17239. Queries made: 463\n",
      "Processed 488 / 17239. Queries made: 464\n",
      "Processed 489 / 17239. Queries made: 465\n",
      "Processed 490 / 17239. Queries made: 466\n",
      "Processed 491 / 17239. Queries made: 467\n",
      "Processed 492 / 17239. Queries made: 468\n",
      "Processed 493 / 17239. Queries made: 469\n",
      "Processed 494 / 17239. Queries made: 470\n",
      "Processed 495 / 17239. Queries made: 471\n",
      "Processed 496 / 17239. Queries made: 472\n",
      "Processed 497 / 17239. Queries made: 473\n",
      "Processed 498 / 17239. Queries made: 474\n",
      "Processed 499 / 17239. Queries made: 475\n",
      "Processed 500 / 17239. Queries made: 476\n",
      "Processed 501 / 17239. Queries made: 477\n",
      "Processed 502 / 17239. Queries made: 478\n",
      "Processed 503 / 17239. Queries made: 479\n",
      "Processed 504 / 17239. Queries made: 480\n",
      "Processed 505 / 17239. Queries made: 481\n",
      "Processed 506 / 17239. Queries made: 482\n",
      "Processed 507 / 17239. Queries made: 483\n",
      "Processed 509 / 17239. Queries made: 484\n",
      "Processed 510 / 17239. Queries made: 485\n",
      "Processed 511 / 17239. Queries made: 486\n",
      "Processed 512 / 17239. Queries made: 487\n",
      "Processed 513 / 17239. Queries made: 488\n",
      "Processed 514 / 17239. Queries made: 489\n",
      "Processed 515 / 17239. Queries made: 490\n",
      "Processed 516 / 17239. Queries made: 491\n",
      "Processed 517 / 17239. Queries made: 492\n",
      "Processed 518 / 17239. Queries made: 493\n",
      "Processed 519 / 17239. Queries made: 494\n",
      "Processed 520 / 17239. Queries made: 495\n",
      "Processed 521 / 17239. Queries made: 496\n",
      "Processed 522 / 17239. Queries made: 497\n",
      "Processed 523 / 17239. Queries made: 498\n",
      "Processed 524 / 17239. Queries made: 499\n",
      "Processed 525 / 17239. Queries made: 500\n",
      "Processed 526 / 17239. Queries made: 501\n",
      "Processed 527 / 17239. Queries made: 502\n",
      "Processed 528 / 17239. Queries made: 503\n",
      "Processed 529 / 17239. Queries made: 504\n",
      "Processed 530 / 17239. Queries made: 505\n",
      "Processed 531 / 17239. Queries made: 506\n",
      "Processed 532 / 17239. Queries made: 507\n",
      "Processed 533 / 17239. Queries made: 508\n",
      "Processed 534 / 17239. Queries made: 509\n",
      "Processed 535 / 17239. Queries made: 510\n",
      "Processed 536 / 17239. Queries made: 511\n",
      "Processed 537 / 17239. Queries made: 512\n",
      "Processed 538 / 17239. Queries made: 513\n",
      "Processed 539 / 17239. Queries made: 514\n",
      "Processed 540 / 17239. Queries made: 515\n",
      "Processed 541 / 17239. Queries made: 516\n",
      "Processed 542 / 17239. Queries made: 517\n",
      "Processed 543 / 17239. Queries made: 518\n",
      "Processed 544 / 17239. Queries made: 519\n",
      "Processed 545 / 17239. Queries made: 520\n",
      "Processed 546 / 17239. Queries made: 521\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "# Assuming 'df' is your DataFrame with at least 'SearchQuery' and 'EntityNumber' columns\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "@sleep_and_retry\n",
    "@limits(calls=20, period=1)\n",
    "def google_search(query, api_key, cse_id, **kwargs):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}\"\n",
    "    response = requests.get(url, params=kwargs)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    \n",
    "    results = google_search(search_query, api_key, cse_id, num=max_results)\n",
    "    for item in results.get('items', []):\n",
    "        url = item['link']\n",
    "        if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "                \n",
    "    return top_urls\n",
    "\n",
    "# Function to integrate all functionalities\n",
    "def main():\n",
    "    # Load or initialize progress tracking\n",
    "    try:\n",
    "        progress_df = pd.read_csv('search_results_Google.csv')\n",
    "        collected_data = progress_df.to_dict('records')\n",
    "        query_count = len(collected_data)\n",
    "    except FileNotFoundError:\n",
    "        collected_data = []\n",
    "        query_count = 0\n",
    "\n",
    "    # Define total_rows\n",
    "    total_rows = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if query_count >= 10000:\n",
    "            print(\"Daily query limit reached. Please resume tomorrow.\")\n",
    "            break\n",
    "        \n",
    "        if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "            continue\n",
    "        \n",
    "        search_query = row['SearchQuery']\n",
    "        entity_number = row['EntityNumber']\n",
    "        filtered_urls = scrape_top_urls_google(search_query, [], 5)\n",
    "        \n",
    "        collected_data.append({\"EntityNumber\": entity_number, \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\",\n",
    "                               \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\",\n",
    "                               \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\",\n",
    "                               \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\",\n",
    "                               \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"})\n",
    "        \n",
    "        query_count += 1\n",
    "        if (index % 100 == 0 or index == total_rows - 1) and collected_data:  # Save progress intermittently and at the end\n",
    "            pd.DataFrame(collected_data).to_csv('search_results_Google.csv', index=False)\n",
    "        \n",
    "        # Optional: Display progress\n",
    "        print(f\"Processed {index + 1} / {total_rows}. Queries made: {query_count}\")\n",
    "        \n",
    "    if query_count < 10000:\n",
    "        print(\"All data has been processed and saved.\")\n",
    "    else:\n",
    "        print(\"Reached the daily limit of queries. Please resume later.\")\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment and call main() when ready\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\curl_cffi\\aio.py:39: RuntimeWarning: \n",
      "    Proactor event loop does not implement add_reader family of methods required.\n",
      "    Registering an additional selector thread for add_reader support.\n",
      "    To avoid this warning use:\n",
      "        asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())\n",
      "    \n",
      "  warnings.warn(PROACTOR_WARNING, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:94\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_url\u001b[1;34m(self, method, url, data, params)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_500_in_url(resp\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;129;01mor\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m202\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRatelimit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m: Ratelimit",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     filtered_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_urls_ddg\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Append entity number and URLs to collected_data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\ratelimit\\decorators.py:113\u001b[0m, in \u001b[0;36msleep_and_retry.<locals>.wrapper\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitException \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\ratelimit\\decorators.py:80\u001b[0m, in \u001b[0;36mRateLimitDecorator.__call__.<locals>.wrapper\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mscrape_top_urls_ddg\u001b[1;34m(search_query, skip_domains, max_results)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:40\u001b[0m, in \u001b[0;36mDDGS.text\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_async_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:36\u001b[0m, in \u001b[0;36mDDGS._run_async_in_thread\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m     35\u001b[0m future: Future[Any] \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:135\u001b[0m, in \u001b[0;36mAsyncDDGS.text\u001b[1;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_api(keywords, region, safesearch, timelimit, max_results)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:214\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api\u001b[1;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[0;32m    213\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mextend(_text_api_page(s, i) \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m23\u001b[39m, max_results, \u001b[38;5;241m50\u001b[39m), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(islice(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, results), max_results))\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:193\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api.<locals>._text_api_page\u001b[1;34m(s, page)\u001b[0m\n\u001b[0;32m    192\u001b[0m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 193\u001b[0m resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://links.duckduckgo.com/d.js\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m    194\u001b[0m page_data \u001b[38;5;241m=\u001b[39m _text_extract_json(resp_content, keywords)\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:98\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_url\u001b[1;34m(self, method, url, data, params)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_aget_url() \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ex)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_aget_url() \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m return None. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit or other error encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Waiting for 1 minute before retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for 1 minute\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Retry the current iteration\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Convert collected data to DataFrame and save after each successful retrieval\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions.\n",
    "#%pip install duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=2)\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "\n",
    "# List to collect rows, or load existing progress if restarting script\n",
    "try:\n",
    "    # Try loading existing progress if this script is being restarted\n",
    "    result_df = pd.read_csv('search_results_DDG.csv')\n",
    "    collected_data = result_df.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    # If no existing data, start fresh\n",
    "    collected_data = []\n",
    "\n",
    "# Get the total number of rows for progress calculation\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Check if this query has already been processed\n",
    "    if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "        continue  # Skip this row if already processed\n",
    "    \n",
    "    search_query = row['SearchQuery']\n",
    "    entity_number = row['EntityNumber']\n",
    "    try:\n",
    "        filtered_urls = scrape_top_urls_ddg(search_query, skip_domains=[], max_results=5)\n",
    "        # Append entity number and URLs to collected_data\n",
    "        collected_data.append({\"EntityNumber\": entity_number, \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\", \n",
    "                               \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\", \n",
    "                               \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\", \n",
    "                               \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\", \n",
    "                               \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Rate limit or other error encountered: {e}. Waiting for 1 minute before retrying...\")\n",
    "        time.sleep(60)  # Wait for 1 minute\n",
    "        continue  # Retry the current iteration\n",
    "        \n",
    "    # Convert collected data to DataFrame and save after each successful retrieval\n",
    "    result_df = pd.DataFrame(collected_data)\n",
    "    result_df.to_csv('search_results_DDG.csv', index=False)\n",
    "    \n",
    "    # Progress feedback\n",
    "    progress_percentage = ((index + 1) / total_rows) * 100\n",
    "    print(f\"Progress: {progress_percentage:.2f}% ({index + 1}/{total_rows})\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing search\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs from Bing:\n",
      "https://iglimburg.be/contact/\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/over-ons\n",
      "https://www.terheide.be/\n",
      "https://publiek.departementzorg.be/Cobrha/Institutions/Institution/WVG_VAPH/201330/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_config():\n",
    "    with open('config.json') as config_file:\n",
    "        return json.load(config_file)\n",
    "\n",
    "def bing_search(query, api_key, endpoint):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n",
    "    params = {\"q\": query, \"count\": 5}  # Adjust count as necessary\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_bing(search_query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    top_urls = []\n",
    "\n",
    "    results = bing_search(search_query, config['BING_API_KEY'], config['BING_ENDPOINT'])\n",
    "\n",
    "    # Filter and collect URLs\n",
    "    for result in results.get('webPages', {}).get('value', []):\n",
    "        url = result.get('url')\n",
    "        if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "            \n",
    "    return top_urls\n",
    "\n",
    "# Example usage\n",
    "# Assume 'df' is your DataFrame and 'skip_domains' are defined as shown earlier\n",
    "if 'df' in locals():\n",
    "    first_search_query = df.iloc[0]['SearchQuery']\n",
    "    filtered_urls = scrape_top_urls_bing(first_search_query, skip_domains)\n",
    "\n",
    "    print(\"Filtered URLs from Bing:\")\n",
    "    for url in filtered_urls:\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutli-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays all unfiltered URLs results. Has duplicates regarding same website but to different pages on same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with scores and penalties:\n",
      "https://www.alkover.be/ - Score: 1.33\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.alkover.be/chalets - Score: 12.0\n",
      "https://www.alkover.be/tenten - Score: 12.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    # Perform searches\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    # Initialize storage for aggregated results\n",
    "    all_results = {}\n",
    "    for url in set(google_results + bing_results + ddg_results):\n",
    "        all_results[url] = {\n",
    "            'ranks': [],\n",
    "            'appearances': 0\n",
    "        }\n",
    "    \n",
    "    # Assign ranks and count appearances\n",
    "    for rank, url in enumerate(google_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(bing_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(ddg_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "\n",
    "    # Calculate scores with penalties\n",
    "    scored_urls = {}\n",
    "    for url, data in all_results.items():\n",
    "        mean_rank = sum(data['ranks']) / len(data['ranks'])\n",
    "        # Apply penalty based on the number of search engines the URL appeared in\n",
    "        penalty = 0\n",
    "        if data['appearances'] == 2:\n",
    "            penalty = 5  # Missing in one search engine\n",
    "        elif data['appearances'] == 1:\n",
    "            penalty = 10  # Missing in two search engines\n",
    "        scored_urls[url] = mean_rank + penalty\n",
    "\n",
    "    # Sort URLs by their score\n",
    "    sorted_urls = sorted(scored_urls.items(), key=lambda item: item[1])\n",
    "\n",
    "    # Take the top N results and prepare them for display\n",
    "    top_urls_with_scores = [(url, round(score, 2)) for url, score in sorted_urls][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "# Usage\n",
    "query = df.iloc[1456]['SearchQuery']\n",
    "max_results = 5  # Adjust as necessary\n",
    "top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results)\n",
    "\n",
    "print(\"Top URLs from multi-search with scores and penalties:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only displays unique website URLs. Not the same websites leading to different pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with domain aggregation and scores:\n",
      "https://www.alkover.be/tenten - Score: 2.7142857142857144\n",
      "https://www.openingsuren.vlaanderen/alkover/8610-kortemark/ieperstraat-17 - Score: 8.5\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 12.0\n",
      "https://www.eventplanner.be/bedrijven/2800_alkover.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_domain(url):\n",
    "    \"\"\"Extracts domain from a URL.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "def perform_multi_search_with_aggregation(query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    \n",
    "    # Conduct searches across all engines\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "    \n",
    "    # Aggregate all URLs and their source ranks\n",
    "    urls_info = collections.defaultdict(lambda: {\"ranks\": [], \"appearances\": 0})\n",
    "    for index, url in enumerate(google_results + bing_results + ddg_results):\n",
    "        domain = get_domain(url)\n",
    "        urls_info[domain]['ranks'].append(index % max_results + 1)\n",
    "        urls_info[domain]['appearances'] += 1\n",
    "        if 'url' not in urls_info[domain] or index % max_results == 0:\n",
    "            urls_info[domain]['url'] = url  # Prioritize URLs by first occurrence\n",
    "\n",
    "    # Calculate scores and apply penalties\n",
    "    for domain, info in urls_info.items():\n",
    "        base_score = sum(info['ranks']) / len(info['ranks'])\n",
    "        penalty = 0\n",
    "        if info['appearances'] < 3:  # Apply penalty if not found by all engines\n",
    "            penalty = 5 * (3 - info['appearances'])\n",
    "        info['score'] = base_score + penalty\n",
    "\n",
    "    # Sort domains by their scores\n",
    "    sorted_domains = sorted(urls_info.values(), key=lambda x: x['score'])\n",
    "\n",
    "    # Prepare top N results\n",
    "    top_urls_with_scores = [(info['url'], info['score']) for info in sorted_domains][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "\n",
    "query = df.iloc[1456]['SearchQuery'] \n",
    "top_urls_with_scores = perform_multi_search_with_aggregation(query, skip_domains, 5)\n",
    "\n",
    "print(\"Top URLs from multi-search with domain aggregation and scores:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
