{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: lxml>=5.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from click>=8.1.7->duckduckgo_search->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                       OfficialName ZipCode  \\\n",
      "0  0201.310.929                                                IGL    3600   \n",
      "1  0202.239.951                                           PROXIMUS    1030   \n",
      "2  0203.201.340                          Nationale Bank van België    1000   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...    9100   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...    1210   \n",
      "\n",
      "          Municipality                Street HouseNumber  \\\n",
      "0                 Genk            Klotstraat         125   \n",
      "1           Schaarbeek  Koning AlbertII laan          27   \n",
      "2              Brussel     de Berlaimontlaan          14   \n",
      "3         Sint-Niklaas             Lamstraat         113   \n",
      "4  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              SearchQuery\n",
      "0                                                                    Website IGL 3600 Genk Klotstraat 125\n",
      "1                                                Website PROXIMUS 1030 Schaarbeek Koning AlbertII laan 27\n",
      "2                                     Website Nationale Bank van België 1000 Brussel de Berlaimontlaan 14\n",
      "3    Website Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas Lamstraat 113\n",
      "4  Website Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node Galileelaan 5\n",
      "Website DATACTION 9320 Aalst Ninovesteenweg 198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"Website {row['OfficialName']} {row['ZipCode']} {row['Municipality']} {row['Street']} {row['HouseNumber']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())\n",
    "print(df.iloc[1600]['SearchQuery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org', 'trendstop.levif.be', 'www.companyweb.be', 'www.linkedin.com', 'https://www.companyweb.be', 'https://bizzy.org', 'https://www.linkedin.com', 'https://fincheck.be', 'https://en.wikipedia.org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5102 / 17239. Queries made: 4694\n",
      "Processed 5103 / 17239. Queries made: 4695\n",
      "Processed 5104 / 17239. Queries made: 4696\n",
      "Processed 5105 / 17239. Queries made: 4697\n",
      "Processed 5106 / 17239. Queries made: 4698\n",
      "Processed 5107 / 17239. Queries made: 4699\n",
      "Processed 5109 / 17239. Queries made: 4700\n",
      "Processed 5110 / 17239. Queries made: 4701\n",
      "Processed 5111 / 17239. Queries made: 4702\n",
      "Processed 5112 / 17239. Queries made: 4703\n",
      "Processed 5113 / 17239. Queries made: 4704\n",
      "Processed 5114 / 17239. Queries made: 4705\n",
      "Processed 5115 / 17239. Queries made: 4706\n",
      "Processed 5116 / 17239. Queries made: 4707\n",
      "Processed 5117 / 17239. Queries made: 4708\n",
      "Processed 5118 / 17239. Queries made: 4709\n",
      "Processed 5119 / 17239. Queries made: 4710\n",
      "Processed 5120 / 17239. Queries made: 4711\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?q=Website%20&num=5#NotOnlyIdeas%203090%20Overijse%20Vlierbeekberg%2090&key=AIzaSyCUgsOA7l1UF5FLqf8HcYGj77_1fbvMjCo&cx=547b992564d834d45",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 82\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReached the daily limit of queries. Please resume later.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Uncomment and call main() when ready\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 59\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m search_query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearchQuery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m entity_number \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 59\u001b[0m filtered_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_urls_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m collected_data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: entity_number, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL1\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL2\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     63\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL3\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL4\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL5\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     67\u001b[0m query_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mscrape_top_urls_google\u001b[1;34m(search_query, skip_domains, max_results)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_top_urls_google\u001b[39m(search_query, skip_domains, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     23\u001b[0m     top_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 25\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcse_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m, []):\n\u001b[0;32m     27\u001b[0m         url \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\ratelimit\\decorators.py:113\u001b[0m, in \u001b[0;36msleep_and_retry.<locals>.wrapper\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitException \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    115\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(exception\u001b[38;5;241m.\u001b[39mperiod_remaining)\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\ratelimit\\decorators.py:80\u001b[0m, in \u001b[0;36mRateLimitDecorator.__call__.<locals>.wrapper\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RateLimitException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many calls\u001b[39m\u001b[38;5;124m'\u001b[39m, period_remaining)\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mgoogle_search\u001b[1;34m(query, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/customsearch/v1?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&cx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcse_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?q=Website%20&num=5#NotOnlyIdeas%203090%20Overijse%20Vlierbeekberg%2090&key=AIzaSyCUgsOA7l1UF5FLqf8HcYGj77_1fbvMjCo&cx=547b992564d834d45"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "@sleep_and_retry\n",
    "@limits(calls=20, period=1)\n",
    "def google_search(query, api_key, cse_id, **kwargs):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}\"\n",
    "    response = requests.get(url, params=kwargs)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    \n",
    "    results = google_search(search_query, api_key, cse_id, num=max_results)\n",
    "    for item in results.get('items', []):\n",
    "        url = item['link']\n",
    "        if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "                \n",
    "    return top_urls\n",
    "\n",
    "# Function to integrate all functionalities\n",
    "def main():\n",
    "    # Load or initialize progress tracking\n",
    "    try:\n",
    "        progress_df = pd.read_csv('search_results_Google.csv')\n",
    "        collected_data = progress_df.to_dict('records')\n",
    "        query_count = len(collected_data)\n",
    "    except FileNotFoundError:\n",
    "        collected_data = []\n",
    "        query_count = 0\n",
    "\n",
    "    # Define total_rows\n",
    "    total_rows = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if query_count >= 10000:\n",
    "            print(\"Daily query limit reached. Please resume tomorrow.\")\n",
    "            break\n",
    "        \n",
    "        if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "            continue\n",
    "        \n",
    "        search_query = row['SearchQuery']\n",
    "        entity_number = row['EntityNumber']\n",
    "        filtered_urls = scrape_top_urls_google(search_query, [], 5)\n",
    "        \n",
    "        collected_data.append({\"EntityNumber\": entity_number, \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\",\n",
    "                               \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\",\n",
    "                               \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\",\n",
    "                               \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\",\n",
    "                               \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"})\n",
    "        \n",
    "        query_count += 1\n",
    "        if (index % 100 == 0 or index == total_rows - 1) and collected_data:  # Save progress intermittently and at the end\n",
    "            pd.DataFrame(collected_data).to_csv('search_results_Google.csv', index=False)\n",
    "        \n",
    "        # Optional: Display progress\n",
    "        print(f\"Processed {index + 1} / {total_rows}. Queries made: {query_count}\")\n",
    "        \n",
    "    if query_count < 10000:\n",
    "        print(\"All data has been processed and saved.\")\n",
    "    else:\n",
    "        print(\"Reached the daily limit of queries. Please resume later.\")\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment and call main() when ready\n",
    "main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.09% (15/17239)\n",
      "Progress: 0.09% (16/17239)\n",
      "Progress: 0.10% (17/17239)\n",
      "Progress: 0.10% (18/17239)\n",
      "Progress: 0.12% (20/17239)\n",
      "Progress: 0.12% (21/17239)\n",
      "Progress: 0.13% (22/17239)\n",
      "Progress: 0.13% (23/17239)\n",
      "Progress: 0.15% (25/17239)\n",
      "Progress: 0.16% (27/17239)\n",
      "Progress: 0.16% (28/17239)\n",
      "Progress: 0.17% (29/17239)\n",
      "Progress: 0.17% (30/17239)\n",
      "Progress: 0.19% (32/17239)\n",
      "Progress: 0.19% (33/17239)\n",
      "Progress: 0.20% (34/17239)\n",
      "Progress: 0.21% (37/17239)\n",
      "Progress: 0.23% (40/17239)\n",
      "Progress: 0.31% (54/17239)\n",
      "Progress: 0.32% (56/17239)\n",
      "Progress: 0.33% (57/17239)\n",
      "Progress: 0.34% (58/17239)\n",
      "Progress: 0.46% (79/17239)\n",
      "Progress: 0.48% (82/17239)\n",
      "Progress: 0.49% (84/17239)\n",
      "Progress: 0.53% (92/17239)\n",
      "Progress: 0.54% (93/17239)\n",
      "Progress: 0.56% (96/17239)\n",
      "Progress: 0.56% (97/17239)\n",
      "Progress: 0.58% (100/17239)\n",
      "Progress: 0.60% (103/17239)\n",
      "Progress: 0.60% (104/17239)\n",
      "Progress: 0.68% (117/17239)\n",
      "Progress: 0.69% (119/17239)\n",
      "Progress: 0.70% (121/17239)\n",
      "Progress: 0.73% (125/17239)\n",
      "Progress: 0.73% (126/17239)\n",
      "Progress: 0.74% (127/17239)\n",
      "Progress: 0.74% (128/17239)\n",
      "Progress: 0.77% (132/17239)\n",
      "Progress: 0.78% (135/17239)\n",
      "Progress: 0.80% (138/17239)\n",
      "Progress: 0.82% (141/17239)\n",
      "Progress: 0.82% (142/17239)\n",
      "Progress: 0.83% (143/17239)\n",
      "Progress: 0.84% (144/17239)\n",
      "Progress: 0.84% (145/17239)\n",
      "Progress: 0.85% (146/17239)\n",
      "Progress: 0.85% (147/17239)\n",
      "Progress: 0.86% (148/17239)\n",
      "Progress: 0.86% (149/17239)\n",
      "Progress: 0.87% (150/17239)\n",
      "Progress: 0.88% (151/17239)\n",
      "Progress: 0.90% (155/17239)\n",
      "Progress: 0.90% (156/17239)\n",
      "Progress: 0.91% (157/17239)\n",
      "Progress: 0.92% (158/17239)\n",
      "Progress: 0.92% (159/17239)\n",
      "Progress: 0.93% (160/17239)\n",
      "Progress: 0.93% (161/17239)\n",
      "Progress: 0.94% (162/17239)\n",
      "Progress: 0.95% (163/17239)\n",
      "Progress: 0.95% (164/17239)\n",
      "Progress: 0.96% (165/17239)\n",
      "Progress: 0.96% (166/17239)\n",
      "Progress: 0.97% (167/17239)\n",
      "Progress: 0.97% (168/17239)\n",
      "Progress: 0.98% (169/17239)\n",
      "Progress: 0.99% (170/17239)\n",
      "Progress: 0.99% (171/17239)\n",
      "Progress: 1.00% (172/17239)\n",
      "Progress: 1.00% (173/17239)\n",
      "Progress: 1.01% (174/17239)\n",
      "Progress: 1.02% (175/17239)\n",
      "Progress: 1.02% (176/17239)\n",
      "Progress: 1.03% (177/17239)\n",
      "Progress: 1.03% (178/17239)\n",
      "Progress: 1.04% (179/17239)\n",
      "Progress: 1.04% (180/17239)\n",
      "Progress: 1.05% (181/17239)\n",
      "Progress: 1.06% (182/17239)\n",
      "Progress: 1.06% (183/17239)\n",
      "Progress: 1.07% (184/17239)\n",
      "Progress: 1.07% (185/17239)\n",
      "Progress: 1.08% (186/17239)\n",
      "Progress: 1.08% (187/17239)\n",
      "Progress: 1.09% (188/17239)\n",
      "Progress: 1.10% (189/17239)\n",
      "Progress: 1.10% (190/17239)\n",
      "Progress: 1.11% (191/17239)\n",
      "Progress: 1.11% (192/17239)\n",
      "Progress: 1.12% (193/17239)\n",
      "Progress: 1.14% (197/17239)\n",
      "Progress: 1.15% (198/17239)\n",
      "Progress: 1.15% (199/17239)\n",
      "Progress: 1.16% (200/17239)\n",
      "Progress: 1.17% (201/17239)\n",
      "Progress: 1.17% (202/17239)\n",
      "Progress: 1.18% (203/17239)\n",
      "Progress: 1.18% (204/17239)\n",
      "Progress: 1.19% (205/17239)\n",
      "Progress: 1.19% (206/17239)\n",
      "Progress: 1.20% (207/17239)\n",
      "Progress: 1.21% (208/17239)\n",
      "Progress: 1.21% (209/17239)\n",
      "Progress: 1.22% (210/17239)\n",
      "Progress: 1.22% (211/17239)\n",
      "Progress: 1.23% (212/17239)\n",
      "Progress: 1.24% (213/17239)\n",
      "Progress: 1.24% (214/17239)\n",
      "Progress: 1.25% (215/17239)\n",
      "Progress: 1.25% (216/17239)\n",
      "Progress: 1.26% (217/17239)\n",
      "Progress: 1.26% (218/17239)\n",
      "Progress: 1.27% (219/17239)\n",
      "Progress: 1.28% (220/17239)\n",
      "Progress: 1.28% (221/17239)\n",
      "Progress: 1.29% (222/17239)\n",
      "Progress: 1.29% (223/17239)\n",
      "Progress: 1.30% (224/17239)\n",
      "Progress: 1.31% (225/17239)\n",
      "Progress: 1.32% (227/17239)\n",
      "Progress: 1.32% (228/17239)\n",
      "Progress: 1.33% (229/17239)\n",
      "Progress: 1.33% (230/17239)\n",
      "Progress: 1.34% (231/17239)\n",
      "Progress: 1.35% (232/17239)\n",
      "Progress: 1.35% (233/17239)\n",
      "Progress: 1.36% (235/17239)\n",
      "Progress: 1.37% (236/17239)\n",
      "Progress: 1.37% (237/17239)\n",
      "Progress: 1.38% (238/17239)\n",
      "Progress: 1.39% (239/17239)\n",
      "Progress: 1.39% (240/17239)\n",
      "Progress: 1.40% (241/17239)\n",
      "Progress: 1.40% (242/17239)\n",
      "Progress: 1.41% (243/17239)\n",
      "Progress: 1.42% (244/17239)\n",
      "Progress: 1.44% (248/17239)\n",
      "Progress: 1.44% (249/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.46% (251/17239)\n",
      "Progress: 1.46% (252/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.47% (254/17239)\n",
      "Progress: 1.48% (255/17239)\n",
      "Progress: 1.49% (256/17239)\n",
      "Progress: 1.49% (257/17239)\n",
      "Progress: 1.50% (258/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.51% (260/17239)\n",
      "Progress: 1.51% (261/17239)\n",
      "Progress: 1.52% (262/17239)\n",
      "Progress: 1.53% (263/17239)\n",
      "Progress: 1.53% (264/17239)\n",
      "Progress: 1.54% (265/17239)\n",
      "Progress: 1.54% (266/17239)\n",
      "Progress: 1.55% (267/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.56% (269/17239)\n",
      "Progress: 1.57% (270/17239)\n",
      "Progress: 1.57% (271/17239)\n",
      "Progress: 1.58% (272/17239)\n",
      "Progress: 1.58% (273/17239)\n",
      "Progress: 1.59% (274/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.61% (277/17239)\n",
      "Progress: 1.61% (278/17239)\n",
      "Progress: 1.62% (279/17239)\n",
      "Progress: 1.62% (280/17239)\n",
      "Progress: 1.63% (281/17239)\n",
      "Progress: 1.64% (282/17239)\n",
      "Progress: 1.64% (283/17239)\n",
      "Progress: 1.65% (284/17239)\n",
      "Progress: 1.65% (285/17239)\n",
      "Progress: 1.66% (286/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.67% (288/17239)\n",
      "Progress: 1.68% (289/17239)\n",
      "Progress: 1.68% (290/17239)\n",
      "Progress: 1.69% (291/17239)\n",
      "Progress: 1.69% (292/17239)\n",
      "Progress: 1.70% (293/17239)\n",
      "Progress: 1.71% (294/17239)\n",
      "Progress: 1.71% (295/17239)\n",
      "Progress: 1.72% (296/17239)\n",
      "Progress: 1.72% (297/17239)\n",
      "Progress: 1.73% (298/17239)\n",
      "Progress: 1.73% (299/17239)\n",
      "Progress: 1.74% (300/17239)\n",
      "Progress: 1.75% (301/17239)\n",
      "Progress: 1.75% (302/17239)\n",
      "Progress: 1.76% (303/17239)\n",
      "Progress: 1.76% (304/17239)\n",
      "Progress: 1.77% (305/17239)\n",
      "Progress: 1.78% (306/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.80% (310/17239)\n",
      "Progress: 1.80% (311/17239)\n",
      "Progress: 1.82% (313/17239)\n",
      "Progress: 1.82% (314/17239)\n",
      "Progress: 1.83% (315/17239)\n",
      "Progress: 1.83% (316/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.84% (318/17239)\n",
      "Progress: 1.85% (319/17239)\n",
      "Progress: 1.86% (320/17239)\n",
      "Progress: 1.86% (321/17239)\n",
      "Progress: 1.87% (322/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.88% (324/17239)\n",
      "Progress: 1.89% (325/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.90% (327/17239)\n",
      "Progress: 1.90% (328/17239)\n",
      "Progress: 1.91% (329/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.92% (331/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.93% (333/17239)\n",
      "Progress: 1.94% (334/17239)\n",
      "Progress: 1.94% (335/17239)\n",
      "Progress: 1.95% (336/17239)\n",
      "Progress: 1.95% (337/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 1.98% (341/17239)\n",
      "Progress: 1.98% (342/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 2.01% (347/17239)\n",
      "Progress: 2.02% (348/17239)\n",
      "Progress: 2.02% (349/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 2.04% (352/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 2.06% (355/17239)\n",
      "Progress: 2.07% (356/17239)\n",
      "Encountered an error: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit\n",
      "Rate limit or other error encountered: _aget_url() https://links.duckduckgo.com/d.js DuckDuckGoSearchException: Ratelimit. Waiting for 1 minute before retrying...\n",
      "Progress: 2.08% (358/17239)\n",
      "Progress: 2.08% (359/17239)\n",
      "Progress: 2.09% (360/17239)\n",
      "Progress: 2.09% (361/17239)\n",
      "Progress: 2.10% (362/17239)\n",
      "Progress: 2.11% (363/17239)\n",
      "Progress: 2.11% (364/17239)\n",
      "Progress: 2.12% (365/17239)\n",
      "Progress: 2.12% (366/17239)\n",
      "Progress: 2.13% (367/17239)\n",
      "Progress: 2.13% (368/17239)\n",
      "Progress: 2.14% (369/17239)\n",
      "Progress: 2.15% (370/17239)\n",
      "Progress: 2.15% (371/17239)\n",
      "Progress: 2.16% (372/17239)\n",
      "Progress: 2.16% (373/17239)\n"
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions.\n",
    "#%pip install duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "@sleep_and_retry\n",
    "@limits(calls=1, period=2)\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "\n",
    "# List to collect rows, or load existing progress if restarting script\n",
    "try:\n",
    "    # Try loading existing progress if this script is being restarted\n",
    "    result_df = pd.read_csv('search_results_DDG.csv')\n",
    "    collected_data = result_df.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    # If no existing data, start fresh\n",
    "    collected_data = []\n",
    "\n",
    "# Get the total number of rows for progress calculation\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Check if this query has already been processed\n",
    "    if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "        continue  # Skip this row if already processed\n",
    "    \n",
    "    search_query = row['SearchQuery']\n",
    "    entity_number = row['EntityNumber']\n",
    "    try:\n",
    "        filtered_urls = scrape_top_urls_ddg(search_query, skip_domains=[], max_results=5)\n",
    "        # Append entity number and URLs to collected_data\n",
    "        collected_data.append({\"EntityNumber\": entity_number, \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\", \n",
    "                               \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\", \n",
    "                               \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\", \n",
    "                               \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\", \n",
    "                               \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Rate limit or other error encountered: {e}. Waiting for 1 minute before retrying...\")\n",
    "        time.sleep(60)  # Wait for 1 minute\n",
    "        continue  # Retry the current iteration\n",
    "        \n",
    "    # Convert collected data to DataFrame and save after each successful retrieval\n",
    "    result_df = pd.DataFrame(collected_data)\n",
    "    result_df.to_csv('search_results_DDG.csv', index=False)\n",
    "    \n",
    "    # Progress feedback\n",
    "    progress_percentage = ((index + 1) / total_rows) * 100\n",
    "    print(f\"Progress: {progress_percentage:.2f}% ({index + 1}/{total_rows})\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing search\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs from Bing:\n",
      "https://iglimburg.be/contact/\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/over-ons\n",
      "https://www.terheide.be/\n",
      "https://publiek.departementzorg.be/Cobrha/Institutions/Institution/WVG_VAPH/201330/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_config():\n",
    "    with open('config.json') as config_file:\n",
    "        return json.load(config_file)\n",
    "\n",
    "def bing_search(query, api_key, endpoint):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n",
    "    params = {\"q\": query, \"count\": 5}  # Adjust count as necessary\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_bing(search_query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    top_urls = []\n",
    "\n",
    "    results = bing_search(search_query, config['BING_API_KEY'], config['BING_ENDPOINT'])\n",
    "\n",
    "    # Filter and collect URLs\n",
    "    for result in results.get('webPages', {}).get('value', []):\n",
    "        url = result.get('url')\n",
    "        if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "            \n",
    "    return top_urls\n",
    "\n",
    "# Example usage\n",
    "# Assume 'df' is your DataFrame and 'skip_domains' are defined as shown earlier\n",
    "if 'df' in locals():\n",
    "    first_search_query = df.iloc[0]['SearchQuery']\n",
    "    filtered_urls = scrape_top_urls_bing(first_search_query, skip_domains)\n",
    "\n",
    "    print(\"Filtered URLs from Bing:\")\n",
    "    for url in filtered_urls:\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutli-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays all unfiltered URLs results. Has duplicates regarding same website but to different pages on same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with scores and penalties:\n",
      "https://www.alkover.be/ - Score: 1.33\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.alkover.be/chalets - Score: 12.0\n",
      "https://www.alkover.be/tenten - Score: 12.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    # Perform searches\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    # Initialize storage for aggregated results\n",
    "    all_results = {}\n",
    "    for url in set(google_results + bing_results + ddg_results):\n",
    "        all_results[url] = {\n",
    "            'ranks': [],\n",
    "            'appearances': 0\n",
    "        }\n",
    "    \n",
    "    # Assign ranks and count appearances\n",
    "    for rank, url in enumerate(google_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(bing_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(ddg_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "\n",
    "    # Calculate scores with penalties\n",
    "    scored_urls = {}\n",
    "    for url, data in all_results.items():\n",
    "        mean_rank = sum(data['ranks']) / len(data['ranks'])\n",
    "        # Apply penalty based on the number of search engines the URL appeared in\n",
    "        penalty = 0\n",
    "        if data['appearances'] == 2:\n",
    "            penalty = 5  # Missing in one search engine\n",
    "        elif data['appearances'] == 1:\n",
    "            penalty = 10  # Missing in two search engines\n",
    "        scored_urls[url] = mean_rank + penalty\n",
    "\n",
    "    # Sort URLs by their score\n",
    "    sorted_urls = sorted(scored_urls.items(), key=lambda item: item[1])\n",
    "\n",
    "    # Take the top N results and prepare them for display\n",
    "    top_urls_with_scores = [(url, round(score, 2)) for url, score in sorted_urls][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "# Usage\n",
    "query = df.iloc[1456]['SearchQuery']\n",
    "max_results = 5  # Adjust as necessary\n",
    "top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results)\n",
    "\n",
    "print(\"Top URLs from multi-search with scores and penalties:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only displays unique website URLs. Not the same websites leading to different pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with domain aggregation and scores:\n",
      "https://www.alkover.be/tenten - Score: 2.7142857142857144\n",
      "https://www.openingsuren.vlaanderen/alkover/8610-kortemark/ieperstraat-17 - Score: 8.5\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 12.0\n",
      "https://www.eventplanner.be/bedrijven/2800_alkover.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_domain(url):\n",
    "    \"\"\"Extracts domain from a URL.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "def perform_multi_search_with_aggregation(query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    \n",
    "    # Conduct searches across all engines\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "    \n",
    "    # Aggregate all URLs and their source ranks\n",
    "    urls_info = collections.defaultdict(lambda: {\"ranks\": [], \"appearances\": 0})\n",
    "    for index, url in enumerate(google_results + bing_results + ddg_results):\n",
    "        domain = get_domain(url)\n",
    "        urls_info[domain]['ranks'].append(index % max_results + 1)\n",
    "        urls_info[domain]['appearances'] += 1\n",
    "        if 'url' not in urls_info[domain] or index % max_results == 0:\n",
    "            urls_info[domain]['url'] = url  # Prioritize URLs by first occurrence\n",
    "\n",
    "    # Calculate scores and apply penalties\n",
    "    for domain, info in urls_info.items():\n",
    "        base_score = sum(info['ranks']) / len(info['ranks'])\n",
    "        penalty = 0\n",
    "        if info['appearances'] < 3:  # Apply penalty if not found by all engines\n",
    "            penalty = 5 * (3 - info['appearances'])\n",
    "        info['score'] = base_score + penalty\n",
    "\n",
    "    # Sort domains by their scores\n",
    "    sorted_domains = sorted(urls_info.values(), key=lambda x: x['score'])\n",
    "\n",
    "    # Prepare top N results\n",
    "    top_urls_with_scores = [(info['url'], info['score']) for info in sorted_domains][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "\n",
    "query = df.iloc[1456]['SearchQuery'] \n",
    "top_urls_with_scores = perform_multi_search_with_aggregation(query, skip_domains, 5)\n",
    "\n",
    "print(\"Top URLs from multi-search with domain aggregation and scores:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
