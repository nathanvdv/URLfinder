{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                                  OfficialName  \\\n",
      "0  0201.310.929                                                           IGL   \n",
      "1  0202.239.951                                                      PROXIMUS   \n",
      "2  0203.201.340                                     Nationale Bank van België   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het Land van Waas   \n",
      "4  0206.653.946       Rijksinstituut voor Ziekte- en Invaliditeitsverzekering   \n",
      "\n",
      "  ZipCode         Municipality                Street HouseNumber  \\\n",
      "0    3600                 Genk            Klotstraat         125   \n",
      "1    1030           Schaarbeek  Koning AlbertII laan          27   \n",
      "2    1000              Brussel     de Berlaimontlaan          14   \n",
      "3    9100         Sint-Niklaas             Lamstraat         113   \n",
      "4    1210  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              SearchQuery\n",
      "0                                                                    Website IGL 3600 Genk Klotstraat 125\n",
      "1                                                Website PROXIMUS 1030 Schaarbeek Koning AlbertII laan 27\n",
      "2                                     Website Nationale Bank van België 1000 Brussel de Berlaimontlaan 14\n",
      "3    Website Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas Lamstraat 113\n",
      "4  Website Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node Galileelaan 5\n",
      "Website DATACTION 9320 Aalst Ninovesteenweg 198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"Website {row['OfficialName']} {row['ZipCode']} {row['Municipality']} {row['Street']} {row['HouseNumber']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())\n",
    "print(df.iloc[1600]['SearchQuery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs:\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/\n",
      "https://iglimburg.be/contact/\n",
      "https://app.akov.be/pls/pakov/f?p=INSP_PUBLIEK:VERSLAGEN::DOWNLOAD:::P1000_DLSEC_BLOB_ID:5596\n",
      "https://www.desocialekaart.be/fiches/7241847ec66a49801840a21ebb5c9eca8f8ac8b6bf162a15f978edbe96ab1789/administratieve-gegevens\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to use Google's Custom Search JSON API\n",
    "def google_search(query, api_key, cse_id, **kwargs):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}\"\n",
    "    response = requests.get(url, params=kwargs)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_google(search_query, search_engine, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    \n",
    "    if search_engine == \"Google\":\n",
    "        with open('config.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "            api_key = config['GOOGLE_API_KEY']\n",
    "            cse_id = config['GOOGLE_CSE_ID']\n",
    "            \n",
    "            \"\"\" Create a config.json file where you copy paste these lines: \n",
    "            {  \n",
    "            \"GOOGLE_API_KEY\": \"your_api_key_here\",\n",
    "            \"GOOGLE_CSE_ID\": \"your_cse_id_here\"\n",
    "            }\n",
    "            \"\"\"\n",
    "        \n",
    "        # Fetch more results initially to account for skipped URLs\n",
    "        results = google_search(search_query, api_key, cse_id, num=max_results)\n",
    "        \n",
    "        # Filter out URLs from domains you want to skip and keep collecting until you have 5 (or run out)\n",
    "        for item in results.get('items', []):\n",
    "            url = item['link']\n",
    "            # Check if URL is from a domain to skip\n",
    "            if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Stop once you have 5 URLs after filtering\n",
    "                if len(top_urls) == 5:\n",
    "                    break\n",
    "            \n",
    "    return top_urls\n",
    "\n",
    "# List of domains to skip\n",
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org', 'trendstop.levif.be', 'www.companyweb.be', 'www.linkedin.com']\n",
    "\n",
    "# Fetch URLs from Google, skipping the specified domains\n",
    "# Example: Using the first search query in your DataFrame for demonstration\n",
    "first_search_query = df.iloc[0]['SearchQuery']\n",
    "filtered_urls = scrape_top_urls_google(first_search_query, \"Google\", skip_domains)\n",
    "\n",
    "# Print the filtered URLs\n",
    "print(\"Filtered URLs:\")\n",
    "for url in filtered_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs:\n",
      "https://iglimburg.be/\n",
      "https://publiek.departementzorg.be/Cobrha/Institutions/Institution/WVG_VAPH/203001/\n",
      "https://opencorporates.com/companies/be/0201310929\n",
      "https://www.creditsafe.com/business-index/en-gb/company/igl-be00000028\n",
      "https://b2bhint.com/en/company/be/igl--0201.310.929\n"
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions.\n",
    "#%pip install duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "    \n",
    "    for result in results:\n",
    "        url = result.get('href')\n",
    "        if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "\n",
    "    return top_urls\n",
    "\n",
    "first_search_query = df.iloc[0]['SearchQuery']\n",
    "filtered_urls = scrape_top_urls_ddg(first_search_query, skip_domains, 5)\n",
    "\n",
    "print(\"Filtered URLs:\")\n",
    "for url in filtered_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing search\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs from Bing:\n",
      "https://iglimburg.be/contact/\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/over-ons\n",
      "https://www.terheide.be/\n",
      "https://publiek.departementzorg.be/Cobrha/Institutions/Institution/WVG_VAPH/201330/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_config():\n",
    "    with open('config.json') as config_file:\n",
    "        return json.load(config_file)\n",
    "\n",
    "def bing_search(query, api_key, endpoint):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n",
    "    params = {\"q\": query, \"count\": 5}  # Adjust count as necessary\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls_bing(search_query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    top_urls = []\n",
    "\n",
    "    results = bing_search(search_query, config['BING_API_KEY'], config['BING_ENDPOINT'])\n",
    "\n",
    "    # Filter and collect URLs\n",
    "    for result in results.get('webPages', {}).get('value', []):\n",
    "        url = result.get('url')\n",
    "        if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "            \n",
    "    return top_urls\n",
    "\n",
    "# Example usage\n",
    "# Assume 'df' is your DataFrame and 'skip_domains' are defined as shown earlier\n",
    "if 'df' in locals():\n",
    "    first_search_query = df.iloc[0]['SearchQuery']\n",
    "    filtered_urls = scrape_top_urls_bing(first_search_query, skip_domains)\n",
    "\n",
    "    print(\"Filtered URLs from Bing:\")\n",
    "    for url in filtered_urls:\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutli-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays all unfiltered URLs results. Has duplicates regarding same website but to different pages on same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with scores and penalties:\n",
      "https://www.alkover.be/ - Score: 1.33\n",
      "https://www.alkover.be/tenten - Score: 7.5\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.alkover.be/chalets - Score: 12.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    # Perform searches\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    # Initialize storage for aggregated results\n",
    "    all_results = {}\n",
    "    for url in set(google_results + bing_results + ddg_results):\n",
    "        all_results[url] = {\n",
    "            'ranks': [],\n",
    "            'appearances': 0\n",
    "        }\n",
    "    \n",
    "    # Assign ranks and count appearances\n",
    "    for rank, url in enumerate(google_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(bing_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(ddg_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "\n",
    "    # Calculate scores with penalties\n",
    "    scored_urls = {}\n",
    "    for url, data in all_results.items():\n",
    "        mean_rank = sum(data['ranks']) / len(data['ranks'])\n",
    "        # Apply penalty based on the number of search engines the URL appeared in\n",
    "        penalty = 0\n",
    "        if data['appearances'] == 2:\n",
    "            penalty = 5  # Missing in one search engine\n",
    "        elif data['appearances'] == 1:\n",
    "            penalty = 10  # Missing in two search engines\n",
    "        scored_urls[url] = mean_rank + penalty\n",
    "\n",
    "    # Sort URLs by their score\n",
    "    sorted_urls = sorted(scored_urls.items(), key=lambda item: item[1])\n",
    "\n",
    "    # Take the top N results and prepare them for display\n",
    "    top_urls_with_scores = [(url, round(score, 2)) for url, score in sorted_urls][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "# Usage\n",
    "query = df.iloc[1456]['SearchQuery']\n",
    "max_results = 5  # Adjust as necessary\n",
    "top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results)\n",
    "\n",
    "print(\"Top URLs from multi-search with scores and penalties:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only displays unique website URLs. Not the same websites leading to different pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top URLs from multi-search with domain aggregation and scores:\n",
      "https://www.alkover.be/tenten - Score: 2.7142857142857144\n",
      "https://www.openingsuren.vlaanderen/alkover/8610-kortemark/ieperstraat-17 - Score: 8.5\n",
      "https://www.eventplanner.net/directory/2800_alkover.html - Score: 11.0\n",
      "https://www.dnb.com/business-directory/company-profiles.alkover.015d6270d338219ba53647f45045714b.html - Score: 12.0\n",
      "https://www.eventplanner.be/bedrijven/2800_alkover.html - Score: 13.0\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import collections\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_domain(url):\n",
    "    \"\"\"Extracts domain from a URL.\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "def perform_multi_search_with_aggregation(query, skip_domains, max_results=5):\n",
    "    config = read_config()\n",
    "    \n",
    "    # Conduct searches across all engines\n",
    "    google_results = scrape_top_urls_google(query, \"Google\", skip_domains, max_results)\n",
    "    bing_results = scrape_top_urls_bing(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "    \n",
    "    # Aggregate all URLs and their source ranks\n",
    "    urls_info = collections.defaultdict(lambda: {\"ranks\": [], \"appearances\": 0})\n",
    "    for index, url in enumerate(google_results + bing_results + ddg_results):\n",
    "        domain = get_domain(url)\n",
    "        urls_info[domain]['ranks'].append(index % max_results + 1)\n",
    "        urls_info[domain]['appearances'] += 1\n",
    "        if 'url' not in urls_info[domain] or index % max_results == 0:\n",
    "            urls_info[domain]['url'] = url  # Prioritize URLs by first occurrence\n",
    "\n",
    "    # Calculate scores and apply penalties\n",
    "    for domain, info in urls_info.items():\n",
    "        base_score = sum(info['ranks']) / len(info['ranks'])\n",
    "        penalty = 0\n",
    "        if info['appearances'] < 3:  # Apply penalty if not found by all engines\n",
    "            penalty = 5 * (3 - info['appearances'])\n",
    "        info['score'] = base_score + penalty\n",
    "\n",
    "    # Sort domains by their scores\n",
    "    sorted_domains = sorted(urls_info.values(), key=lambda x: x['score'])\n",
    "\n",
    "    # Prepare top N results\n",
    "    top_urls_with_scores = [(info['url'], info['score']) for info in sorted_domains][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "\n",
    "query = df.iloc[1456]['SearchQuery'] \n",
    "top_urls_with_scores = perform_multi_search_with_aggregation(query, skip_domains, 5)\n",
    "\n",
    "print(\"Top URLs from multi-search with domain aggregation and scores:\")\n",
    "for url, score in top_urls_with_scores:\n",
    "    print(f\"{url} - Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
