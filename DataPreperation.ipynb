{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.2.2)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: orjson>=3.10.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from click>=8.1.7->duckduckgo_search->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                       OfficialName ZipCode  \\\n",
      "0  0201.310.929                                                IGL    3600   \n",
      "1  0202.239.951                                           PROXIMUS    1030   \n",
      "2  0203.201.340                          Nationale Bank van BelgiÃ«    1000   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...    9100   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...    1210   \n",
      "\n",
      "          Municipality                Street HouseNumber  \\\n",
      "0                 Genk            Klotstraat         125   \n",
      "1           Schaarbeek  Koning AlbertII laan          27   \n",
      "2              Brussel     de Berlaimontlaan          14   \n",
      "3         Sint-Niklaas             Lamstraat         113   \n",
      "4  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                        SearchQuery\n",
      "0                                                                     IGL 3600 Genk\n",
      "1                                                          PROXIMUS 1030 Schaarbeek\n",
      "2                                            Nationale Bank van BelgiÃ« 1000 Brussel\n",
      "3    Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas\n",
      "4  Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"{row['OfficialName']} {row['ZipCode']} {row['Municipality']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org'\n",
    "                , 'trendstop.levif.be', 'companyweb.be', 'linkedin.com'\n",
    "                , 'en.wikipedia.org', 'facebook.com', 'be.linkedin.com'\n",
    "                , 'instagram.com', 'werkenbijdeoverheid.be', 'dnb.com', 'nl.wikipedia.org'\n",
    "                , 'youtube.com', 'staatsbladmonitor.be', 'werkenvoor.be'\n",
    "                , 'twitter.com', 'vlaanderen.be/organisaties', 'jobat.be'\n",
    "                , 'vdab.be']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip_domains=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    delay_between_requests = 0.1\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}\"\n",
    "        response = requests.get(url, params=kwargs)\n",
    "        response.raise_for_status()  # Check for HTTP-level issues\n",
    "        data = response.json()  # Parse JSON response\n",
    "        return data\n",
    "    except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "        print(f\"Encountered a network error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, min_results=5, max_retries=1):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) == min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += 10  # Prepare to query next page of results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(0.1)  # Short delay before retrying\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = 'search_results_Google.csv'\n",
    "    placeholder = \"\"  # Placeholder for unfound URLs\n",
    "    batch_size = 10  # Number of queries to process before saving to CSV\n",
    "    processed_batch = []  # Initialize the batch list\n",
    "\n",
    "    try:\n",
    "        progress_df = pd.read_csv(file_path)\n",
    "        last_processed_entity = progress_df['EntityNumber'].max() if not progress_df.empty else None\n",
    "    except FileNotFoundError:\n",
    "        progress_df = pd.DataFrame(columns=['EntityNumber', 'URL1', 'URL2', 'URL3', 'URL4', 'URL5'])\n",
    "        last_processed_entity = None\n",
    "\n",
    "    # Assuming 'df' is your DataFrame with the search queries\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        entity_number = row['EntityNumber']\n",
    "        if last_processed_entity and entity_number <= last_processed_entity:\n",
    "            continue\n",
    "\n",
    "        # Perform search and process results\n",
    "        filtered_urls = scrape_top_urls_google(row['SearchQuery'], skip_domains, 5)\n",
    "        urls_to_add = [filtered_urls[i] if i < len(filtered_urls) else placeholder for i in range(5)]\n",
    "        \n",
    "        processed_batch.append({\"EntityNumber\": entity_number, \"URL1\": urls_to_add[0], \"URL2\": urls_to_add[1], \"URL3\": urls_to_add[2], \"URL4\": urls_to_add[3], \"URL5\": urls_to_add[4]})\n",
    "\n",
    "        if len(processed_batch) >= batch_size or index == total_queries - 1:\n",
    "            # Append batch to DataFrame and reset for next batch\n",
    "            new_rows_df = pd.DataFrame(processed_batch)\n",
    "            progress_df = pd.concat([progress_df, new_rows_df], ignore_index=True)\n",
    "            progress_df.to_csv(file_path, index=False)\n",
    "            processed_batch = []  # Clear the batch\n",
    "\n",
    "        # Update progress\n",
    "        processed_entries = index + 1  # Assuming 'df' is zero-indexed\n",
    "        percentage_completed = (processed_entries / total_queries) * 100\n",
    "        print(f\"Processed EntityNumber {entity_number}. Completion: {percentage_completed:.2f}% [{processed_entries}/{total_queries}]\")\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "# Uncomment and call main() when ready\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\curl_cffi\\aio.py:205: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 68.73% (11848/17239)\n",
      "Progress: 68.73% (11849/17239)\n",
      "Progress: 68.74% (11850/17239)\n",
      "Progress: 68.75% (11851/17239)\n",
      "Progress: 68.76% (11853/17239)\n",
      "Progress: 68.76% (11854/17239)\n",
      "Progress: 68.77% (11855/17239)\n",
      "Progress: 68.77% (11856/17239)\n",
      "Progress: 68.78% (11857/17239)\n",
      "Progress: 68.79% (11858/17239)\n",
      "Progress: 68.81% (11862/17239)\n",
      "Progress: 68.81% (11863/17239)\n",
      "Progress: 68.82% (11864/17239)\n",
      "Progress: 68.83% (11865/17239)\n",
      "Progress: 68.83% (11866/17239)\n",
      "Progress: 68.84% (11867/17239)\n",
      "Progress: 68.84% (11868/17239)\n",
      "Progress: 68.85% (11869/17239)\n",
      "Progress: 68.86% (11870/17239)\n",
      "Progress: 68.86% (11871/17239)\n",
      "Progress: 68.87% (11872/17239)\n",
      "Progress: 68.87% (11873/17239)\n",
      "Progress: 68.88% (11874/17239)\n",
      "Progress: 68.88% (11875/17239)\n",
      "Progress: 68.89% (11876/17239)\n",
      "Progress: 68.90% (11877/17239)\n",
      "Progress: 68.90% (11878/17239)\n",
      "Progress: 68.91% (11879/17239)\n",
      "Progress: 68.91% (11880/17239)\n",
      "Progress: 68.92% (11881/17239)\n",
      "Progress: 68.93% (11882/17239)\n",
      "Progress: 68.93% (11883/17239)\n",
      "Progress: 68.95% (11887/17239)\n",
      "Progress: 68.96% (11888/17239)\n",
      "Progress: 68.97% (11889/17239)\n",
      "Progress: 68.97% (11890/17239)\n",
      "Progress: 68.98% (11891/17239)\n",
      "Progress: 68.98% (11892/17239)\n",
      "Progress: 68.99% (11893/17239)\n",
      "Progress: 68.99% (11894/17239)\n",
      "Progress: 69.00% (11895/17239)\n",
      "Progress: 69.01% (11896/17239)\n",
      "Progress: 69.01% (11897/17239)\n",
      "Progress: 69.02% (11898/17239)\n",
      "Progress: 69.02% (11899/17239)\n",
      "Progress: 69.03% (11900/17239)\n",
      "Progress: 69.04% (11902/17239)\n",
      "Progress: 69.05% (11903/17239)\n",
      "Progress: 69.05% (11904/17239)\n",
      "Progress: 69.06% (11905/17239)\n",
      "Progress: 69.06% (11906/17239)\n",
      "Progress: 69.07% (11907/17239)\n",
      "Progress: 69.08% (11908/17239)\n",
      "Progress: 69.08% (11909/17239)\n",
      "Progress: 69.09% (11910/17239)\n",
      "Progress: 69.09% (11911/17239)\n",
      "Progress: 69.10% (11912/17239)\n",
      "Progress: 69.10% (11913/17239)\n",
      "Progress: 69.11% (11914/17239)\n",
      "Progress: 69.12% (11915/17239)\n",
      "Progress: 69.12% (11916/17239)\n",
      "Progress: 69.13% (11917/17239)\n",
      "Progress: 69.13% (11918/17239)\n",
      "Progress: 69.14% (11919/17239)\n",
      "Progress: 69.15% (11920/17239)\n",
      "Progress: 69.15% (11921/17239)\n",
      "Progress: 69.16% (11922/17239)\n",
      "Progress: 69.16% (11923/17239)\n",
      "Progress: 69.17% (11924/17239)\n",
      "Progress: 69.18% (11926/17239)\n",
      "Progress: 69.19% (11927/17239)\n",
      "Progress: 69.19% (11928/17239)\n",
      "Progress: 69.20% (11929/17239)\n",
      "Progress: 69.20% (11930/17239)\n",
      "Progress: 69.21% (11931/17239)\n",
      "Progress: 69.22% (11932/17239)\n",
      "Progress: 69.22% (11933/17239)\n",
      "Progress: 69.23% (11934/17239)\n",
      "Progress: 69.23% (11935/17239)\n",
      "Progress: 69.24% (11936/17239)\n",
      "Progress: 69.24% (11937/17239)\n",
      "Progress: 69.25% (11938/17239)\n",
      "Progress: 69.26% (11939/17239)\n",
      "Progress: 69.26% (11940/17239)\n",
      "Progress: 69.27% (11941/17239)\n",
      "Progress: 69.27% (11942/17239)\n",
      "Progress: 69.28% (11944/17239)\n",
      "Progress: 69.29% (11945/17239)\n",
      "Progress: 69.30% (11946/17239)\n",
      "Progress: 69.30% (11947/17239)\n",
      "Progress: 69.31% (11948/17239)\n",
      "Progress: 69.31% (11949/17239)\n",
      "Progress: 69.32% (11950/17239)\n",
      "Progress: 69.33% (11951/17239)\n",
      "Progress: 69.33% (11952/17239)\n",
      "Progress: 69.34% (11953/17239)\n",
      "Progress: 69.34% (11954/17239)\n",
      "Progress: 69.35% (11955/17239)\n",
      "Progress: 69.35% (11956/17239)\n",
      "Progress: 69.36% (11957/17239)\n",
      "Progress: 69.37% (11958/17239)\n",
      "Progress: 69.37% (11959/17239)\n",
      "Progress: 69.38% (11960/17239)\n",
      "Progress: 69.38% (11961/17239)\n",
      "Progress: 69.39% (11962/17239)\n",
      "Progress: 69.39% (11963/17239)\n",
      "Progress: 69.40% (11964/17239)\n",
      "Progress: 69.41% (11965/17239)\n",
      "Progress: 69.41% (11966/17239)\n",
      "Progress: 69.42% (11967/17239)\n",
      "Progress: 69.42% (11968/17239)\n",
      "Progress: 69.43% (11969/17239)\n",
      "Progress: 69.44% (11971/17239)\n",
      "Progress: 69.45% (11972/17239)\n",
      "Progress: 69.45% (11973/17239)\n",
      "Progress: 69.46% (11974/17239)\n",
      "Progress: 69.46% (11975/17239)\n",
      "Progress: 69.47% (11976/17239)\n",
      "Progress: 69.48% (11977/17239)\n",
      "Progress: 69.48% (11978/17239)\n",
      "Progress: 69.49% (11979/17239)\n",
      "Progress: 69.49% (11980/17239)\n",
      "Progress: 69.50% (11981/17239)\n",
      "Progress: 69.51% (11982/17239)\n",
      "Progress: 69.51% (11983/17239)\n",
      "Progress: 69.52% (11984/17239)\n",
      "Progress: 69.52% (11985/17239)\n",
      "Progress: 69.53% (11986/17239)\n",
      "Progress: 69.53% (11987/17239)\n",
      "Progress: 69.54% (11988/17239)\n",
      "Progress: 69.55% (11989/17239)\n",
      "Progress: 69.55% (11990/17239)\n",
      "Progress: 69.56% (11991/17239)\n",
      "Progress: 69.56% (11992/17239)\n",
      "Progress: 69.57% (11993/17239)\n",
      "Progress: 69.57% (11994/17239)\n",
      "Progress: 69.58% (11995/17239)\n",
      "Progress: 69.59% (11996/17239)\n",
      "Progress: 69.59% (11997/17239)\n",
      "Progress: 69.60% (11998/17239)\n",
      "Progress: 69.60% (11999/17239)\n",
      "Progress: 69.61% (12000/17239)\n",
      "Progress: 69.62% (12001/17239)\n",
      "Progress: 69.62% (12002/17239)\n",
      "Progress: 69.63% (12003/17239)\n",
      "Progress: 69.63% (12004/17239)\n",
      "Progress: 69.64% (12005/17239)\n",
      "Progress: 69.64% (12006/17239)\n",
      "Progress: 69.65% (12007/17239)\n",
      "Progress: 69.66% (12008/17239)\n",
      "Progress: 69.66% (12009/17239)\n",
      "Progress: 69.67% (12010/17239)\n",
      "Progress: 69.67% (12011/17239)\n",
      "Progress: 69.68% (12012/17239)\n",
      "Progress: 69.69% (12013/17239)\n",
      "Progress: 69.69% (12014/17239)\n",
      "Progress: 69.70% (12016/17239)\n",
      "Progress: 69.71% (12017/17239)\n",
      "Progress: 69.72% (12019/17239)\n",
      "Progress: 69.73% (12020/17239)\n",
      "Progress: 69.73% (12021/17239)\n",
      "Progress: 69.74% (12022/17239)\n",
      "Progress: 69.74% (12023/17239)\n",
      "Progress: 69.75% (12024/17239)\n",
      "Progress: 69.75% (12025/17239)\n",
      "Progress: 69.76% (12026/17239)\n",
      "Progress: 69.77% (12027/17239)\n",
      "Progress: 69.77% (12028/17239)\n",
      "Progress: 69.78% (12029/17239)\n",
      "Progress: 69.78% (12030/17239)\n",
      "Progress: 69.79% (12031/17239)\n",
      "Progress: 69.80% (12032/17239)\n",
      "Progress: 69.80% (12033/17239)\n",
      "Progress: 69.81% (12034/17239)\n",
      "Progress: 69.81% (12035/17239)\n",
      "Progress: 69.82% (12036/17239)\n",
      "Progress: 69.82% (12037/17239)\n",
      "Progress: 69.83% (12038/17239)\n",
      "Progress: 69.84% (12039/17239)\n",
      "Progress: 69.84% (12040/17239)\n",
      "Progress: 69.85% (12041/17239)\n",
      "Progress: 69.85% (12042/17239)\n",
      "Progress: 69.86% (12043/17239)\n",
      "Progress: 69.86% (12044/17239)\n",
      "Progress: 69.87% (12045/17239)\n",
      "Progress: 69.88% (12046/17239)\n",
      "Progress: 69.88% (12047/17239)\n",
      "Progress: 69.89% (12048/17239)\n",
      "Progress: 69.89% (12049/17239)\n",
      "Progress: 69.90% (12050/17239)\n",
      "Progress: 69.91% (12051/17239)\n",
      "Progress: 69.91% (12052/17239)\n",
      "Progress: 69.92% (12053/17239)\n",
      "Progress: 69.92% (12054/17239)\n",
      "Progress: 69.93% (12055/17239)\n",
      "Progress: 69.93% (12056/17239)\n",
      "Progress: 69.94% (12057/17239)\n",
      "Progress: 69.95% (12058/17239)\n",
      "Progress: 69.95% (12059/17239)\n",
      "Progress: 69.96% (12060/17239)\n",
      "Progress: 69.96% (12061/17239)\n",
      "Progress: 69.97% (12062/17239)\n",
      "Progress: 69.98% (12063/17239)\n",
      "Progress: 69.98% (12064/17239)\n",
      "Progress: 69.99% (12065/17239)\n",
      "Progress: 69.99% (12066/17239)\n",
      "Progress: 70.00% (12067/17239)\n",
      "Progress: 70.00% (12068/17239)\n",
      "Progress: 70.01% (12069/17239)\n",
      "Progress: 70.02% (12070/17239)\n",
      "Progress: 70.02% (12071/17239)\n",
      "Progress: 70.03% (12072/17239)\n",
      "Progress: 70.03% (12073/17239)\n",
      "Progress: 70.04% (12075/17239)\n",
      "Progress: 70.05% (12076/17239)\n",
      "Progress: 70.06% (12077/17239)\n",
      "Progress: 70.06% (12078/17239)\n",
      "Progress: 70.07% (12079/17239)\n",
      "Progress: 70.07% (12080/17239)\n",
      "Progress: 70.08% (12081/17239)\n",
      "Progress: 70.09% (12082/17239)\n",
      "Progress: 70.09% (12083/17239)\n",
      "Progress: 70.10% (12084/17239)\n",
      "Progress: 70.10% (12085/17239)\n",
      "Progress: 70.11% (12086/17239)\n",
      "Progress: 70.11% (12087/17239)\n",
      "Progress: 70.12% (12088/17239)\n",
      "Progress: 70.13% (12089/17239)\n",
      "Progress: 70.13% (12090/17239)\n",
      "Progress: 70.14% (12091/17239)\n",
      "Progress: 70.14% (12092/17239)\n",
      "Progress: 70.15% (12093/17239)\n",
      "Progress: 70.15% (12094/17239)\n",
      "Progress: 70.16% (12095/17239)\n",
      "Progress: 70.17% (12096/17239)\n",
      "Progress: 70.17% (12097/17239)\n",
      "Progress: 70.18% (12098/17239)\n",
      "Progress: 70.18% (12099/17239)\n",
      "Progress: 70.19% (12100/17239)\n",
      "Progress: 70.20% (12101/17239)\n",
      "Progress: 70.20% (12102/17239)\n",
      "Progress: 70.21% (12103/17239)\n",
      "Progress: 70.21% (12104/17239)\n",
      "Progress: 70.22% (12105/17239)\n",
      "Progress: 70.22% (12106/17239)\n",
      "Progress: 70.23% (12107/17239)\n",
      "Progress: 70.24% (12108/17239)\n",
      "Progress: 70.24% (12109/17239)\n",
      "Progress: 70.25% (12110/17239)\n",
      "Progress: 70.25% (12111/17239)\n",
      "Progress: 70.26% (12112/17239)\n",
      "Progress: 70.27% (12113/17239)\n",
      "Progress: 70.27% (12114/17239)\n",
      "Progress: 70.28% (12115/17239)\n",
      "Progress: 70.28% (12116/17239)\n",
      "Progress: 70.29% (12117/17239)\n",
      "Progress: 70.29% (12118/17239)\n",
      "Progress: 70.30% (12119/17239)\n",
      "Progress: 70.31% (12121/17239)\n",
      "Progress: 70.32% (12122/17239)\n",
      "Progress: 70.32% (12123/17239)\n",
      "Progress: 70.33% (12125/17239)\n",
      "Progress: 70.34% (12126/17239)\n",
      "Progress: 70.35% (12127/17239)\n",
      "Progress: 70.35% (12128/17239)\n",
      "Progress: 70.36% (12129/17239)\n",
      "Progress: 70.36% (12130/17239)\n",
      "Progress: 70.37% (12131/17239)\n",
      "Progress: 70.38% (12132/17239)\n",
      "Progress: 70.38% (12133/17239)\n",
      "Progress: 70.39% (12134/17239)\n",
      "Progress: 70.39% (12135/17239)\n",
      "Progress: 70.40% (12136/17239)\n",
      "Progress: 70.40% (12137/17239)\n",
      "Progress: 70.41% (12138/17239)\n",
      "Progress: 70.42% (12139/17239)\n",
      "Progress: 70.42% (12140/17239)\n",
      "Progress: 70.43% (12141/17239)\n",
      "Progress: 70.43% (12142/17239)\n",
      "Progress: 70.44% (12143/17239)\n",
      "Progress: 70.44% (12144/17239)\n",
      "Progress: 70.45% (12145/17239)\n",
      "Progress: 70.46% (12146/17239)\n",
      "Progress: 70.46% (12147/17239)\n",
      "Progress: 70.47% (12148/17239)\n",
      "Progress: 70.47% (12149/17239)\n",
      "Progress: 70.48% (12150/17239)\n",
      "Progress: 70.49% (12151/17239)\n",
      "Progress: 70.49% (12152/17239)\n",
      "Progress: 70.50% (12153/17239)\n",
      "Progress: 70.50% (12154/17239)\n",
      "Progress: 70.51% (12155/17239)\n",
      "Progress: 70.51% (12156/17239)\n",
      "Progress: 70.52% (12157/17239)\n",
      "Progress: 70.53% (12158/17239)\n",
      "Progress: 70.53% (12159/17239)\n",
      "Progress: 70.54% (12160/17239)\n",
      "Progress: 70.54% (12161/17239)\n",
      "Progress: 70.55% (12162/17239)\n",
      "Progress: 70.56% (12163/17239)\n",
      "Progress: 70.56% (12164/17239)\n",
      "Progress: 70.57% (12165/17239)\n",
      "Progress: 70.57% (12166/17239)\n",
      "Progress: 70.58% (12167/17239)\n",
      "Progress: 70.58% (12168/17239)\n",
      "Progress: 70.60% (12170/17239)\n",
      "Progress: 70.60% (12171/17239)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m entity_number \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     filtered_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_urls_ddg\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Enforce a simple rate limit\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     collected_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: entity_number, \n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL1\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL5\u001b[39m\u001b[38;5;124m\"\u001b[39m: filtered_urls[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mscrape_top_urls_ddg\u001b[1;34m(search_query, skip_domains, max_results)\u001b[0m\n\u001b[0;32m     14\u001b[0m ddgs \u001b[38;5;241m=\u001b[39m DDGS()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Fetch results with potentially more than needed to account for skipped domains\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     20\u001b[0m         url \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:48\u001b[0m, in \u001b[0;36mDDGS.text\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_async_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:44\u001b[0m, in \u001b[0;36mDDGS._run_async_in_thread\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs an async coroutine in a separate thread.\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m future: Future[Any] \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m---> 44\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\threading.py:334\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions. \n",
    "#%pip install -U duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Decorator to enforce rate limiting\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "try:\n",
    "    result_df = pd.read_csv('search_results_DDG.csv')\n",
    "    collected_data = result_df.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    collected_data = []\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "        continue  # Skip already processed\n",
    "    \n",
    "    search_query = row['SearchQuery']\n",
    "    entity_number = row['EntityNumber']\n",
    "    try:\n",
    "        filtered_urls = scrape_top_urls_ddg(search_query, skip_domains, max_results=5)\n",
    "        time.sleep(1)  # Enforce a simple rate limit\n",
    "\n",
    "        collected_data.append({\n",
    "            \"EntityNumber\": entity_number, \n",
    "            \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\", \n",
    "            \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\", \n",
    "            \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\", \n",
    "            \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\", \n",
    "            \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Waiting before retrying...\")\n",
    "        time.sleep(20)  # Optional: Adjust based on your preference\n",
    "        continue  # Optionally retry the current iteration\n",
    "\n",
    "    result_df = pd.DataFrame(collected_data)\n",
    "    result_df.to_csv('search_results_DDG.csv', index=False)\n",
    "\n",
    "    progress_percentage = ((index + 1) / total_rows) * 100\n",
    "    print(f\"Progress: {progress_percentage:.2f}% ({index + 1}/{total_rows})\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays all unfiltered URLs results. Has duplicates regarding same website but to different pages on same website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 62.93% (10848/17239)\n",
      "Progress: 62.93% (10849/17239)\n",
      "Progress: 62.94% (10850/17239)\n",
      "Progress: 62.94% (10851/17239)\n",
      "Progress: 62.95% (10852/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 62.96% (10853/17239)\n",
      "Progress: 62.96% (10854/17239)\n",
      "Progress: 62.97% (10855/17239)\n",
      "Progress: 62.97% (10856/17239)\n",
      "Progress: 62.98% (10857/17239)\n",
      "Progress: 62.99% (10858/17239)\n",
      "Progress: 62.99% (10859/17239)\n",
      "Progress: 63.00% (10860/17239)\n",
      "Progress: 63.00% (10861/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.01% (10862/17239)\n",
      "Progress: 63.01% (10863/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.02% (10864/17239)\n",
      "Progress: 63.03% (10865/17239)\n",
      "Progress: 63.03% (10866/17239)\n",
      "Progress: 63.05% (10870/17239)\n",
      "Progress: 63.06% (10871/17239)\n",
      "Progress: 63.07% (10872/17239)\n",
      "Progress: 63.07% (10873/17239)\n",
      "Progress: 63.08% (10874/17239)\n",
      "Progress: 63.08% (10875/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.09% (10876/17239)\n",
      "Progress: 63.10% (10877/17239)\n",
      "Progress: 63.11% (10879/17239)\n",
      "Progress: 63.11% (10880/17239)\n",
      "Progress: 63.12% (10881/17239)\n",
      "Progress: 63.12% (10882/17239)\n",
      "Progress: 63.13% (10883/17239)\n",
      "Progress: 63.14% (10884/17239)\n",
      "Progress: 63.14% (10885/17239)\n",
      "Progress: 63.15% (10886/17239)\n",
      "Progress: 63.15% (10887/17239)\n",
      "Progress: 63.16% (10888/17239)\n",
      "Progress: 63.16% (10889/17239)\n",
      "Progress: 63.17% (10890/17239)\n",
      "Progress: 63.18% (10891/17239)\n",
      "Progress: 63.18% (10892/17239)\n",
      "Progress: 63.19% (10893/17239)\n",
      "Progress: 63.19% (10894/17239)\n",
      "Progress: 63.20% (10895/17239)\n",
      "Progress: 63.21% (10896/17239)\n",
      "Progress: 63.21% (10897/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.22% (10898/17239)\n",
      "Progress: 63.22% (10899/17239)\n",
      "Progress: 63.23% (10900/17239)\n",
      "Progress: 63.23% (10901/17239)\n",
      "Progress: 63.24% (10902/17239)\n",
      "Progress: 63.25% (10903/17239)\n",
      "Progress: 63.25% (10904/17239)\n",
      "Progress: 63.26% (10906/17239)\n",
      "Progress: 63.27% (10907/17239)\n",
      "Progress: 63.28% (10908/17239)\n",
      "Progress: 63.28% (10909/17239)\n",
      "Progress: 63.29% (10910/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.29% (10911/17239)\n",
      "Progress: 63.30% (10912/17239)\n",
      "Progress: 63.30% (10913/17239)\n",
      "Progress: 63.31% (10914/17239)\n",
      "Progress: 63.32% (10916/17239)\n",
      "Progress: 63.33% (10917/17239)\n",
      "Progress: 63.33% (10918/17239)\n",
      "Progress: 63.34% (10919/17239)\n",
      "Progress: 63.34% (10920/17239)\n",
      "Progress: 63.35% (10921/17239)\n",
      "Progress: 63.36% (10922/17239)\n",
      "Progress: 63.36% (10923/17239)\n",
      "Progress: 63.37% (10924/17239)\n",
      "Progress: 63.37% (10925/17239)\n",
      "Progress: 63.39% (10927/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.39% (10928/17239)\n",
      "Progress: 63.40% (10929/17239)\n",
      "Progress: 63.40% (10930/17239)\n",
      "Progress: 63.41% (10931/17239)\n",
      "Progress: 63.41% (10932/17239)\n",
      "Progress: 63.42% (10933/17239)\n",
      "Progress: 63.43% (10934/17239)\n",
      "Progress: 63.43% (10935/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.44% (10936/17239)\n",
      "Progress: 63.44% (10937/17239)\n",
      "Progress: 63.45% (10938/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.45% (10939/17239)\n",
      "Progress: 63.46% (10940/17239)\n",
      "Progress: 63.47% (10941/17239)\n",
      "Progress: 63.47% (10942/17239)\n",
      "Progress: 63.48% (10943/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.48% (10944/17239)\n",
      "Progress: 63.49% (10945/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.50% (10946/17239)\n",
      "Progress: 63.50% (10947/17239)\n",
      "Progress: 63.51% (10948/17239)\n",
      "Progress: 63.51% (10949/17239)\n",
      "Progress: 63.52% (10950/17239)\n",
      "Progress: 63.52% (10951/17239)\n",
      "Progress: 63.53% (10952/17239)\n",
      "Progress: 63.54% (10953/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.54% (10954/17239)\n",
      "Progress: 63.55% (10955/17239)\n",
      "Progress: 63.55% (10956/17239)\n",
      "Progress: 63.56% (10957/17239)\n",
      "Progress: 63.57% (10958/17239)\n",
      "Progress: 63.57% (10959/17239)\n",
      "Progress: 63.58% (10960/17239)\n",
      "Progress: 63.58% (10961/17239)\n",
      "Progress: 63.59% (10962/17239)\n",
      "Progress: 63.59% (10963/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.60% (10964/17239)\n",
      "Progress: 63.61% (10965/17239)\n",
      "Progress: 63.61% (10966/17239)\n",
      "No results found in attempt 1. Retrying...\n",
      "Progress: 63.62% (10967/17239)\n",
      "Progress: 63.62% (10968/17239)\n",
      "Progress: 63.63% (10969/17239)\n",
      "Encountered an error: _extract_vqd() keywords='SP!R!T INVEST 8900 Ieper' Could not extract vqd.\n"
     ]
    },
    {
     "ename": "DuckDuckGoSearchException",
     "evalue": "_extract_vqd() keywords='SP!R!T INVEST 8900 Ieper' Could not extract vqd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll data has been processed and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 131\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearchQuery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    129\u001b[0m entity_number \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 131\u001b[0m top_urls_with_scores \u001b[38;5;241m=\u001b[39m \u001b[43mperform_multi_search_with_scores_and_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure you have defined or included skip_domains list\u001b[39;00m\n\u001b[0;32m    132\u001b[0m top_urls \u001b[38;5;241m=\u001b[39m [url \u001b[38;5;28;01mfor\u001b[39;00m url, score \u001b[38;5;129;01min\u001b[39;00m top_urls_with_scores]\n\u001b[0;32m    134\u001b[0m new_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntityNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: entity_number, \n\u001b[0;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL1\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_urls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(top_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL5\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_urls[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(top_urls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m }])\n",
      "Cell \u001b[1;32mIn[8], line 82\u001b[0m, in \u001b[0;36mperform_multi_search_with_scores_and_penalty\u001b[1;34m(query, skip_domains, max_results)\u001b[0m\n\u001b[0;32m     80\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(DELAY_BETWEEN_REQUESTS)\n\u001b[0;32m     81\u001b[0m google_results \u001b[38;5;241m=\u001b[39m scrape_top_urls_google(query, skip_domains, max_results)\n\u001b[1;32m---> 82\u001b[0m ddg_results \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_top_urls_ddg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(google_results \u001b[38;5;241m+\u001b[39m ddg_results):\n",
      "Cell \u001b[1;32mIn[8], line 63\u001b[0m, in \u001b[0;36mscrape_top_urls_ddg\u001b[1;34m(search_query, skip_domains, max_results)\u001b[0m\n\u001b[0;32m     60\u001b[0m ddgs \u001b[38;5;241m=\u001b[39m DDGS()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Fetch results with potentially more than needed to account for skipped domains\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mddgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskip_domains\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     66\u001b[0m         url \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:48\u001b[0m, in \u001b[0;36mDDGS.text\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_async_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search.py:44\u001b[0m, in \u001b[0;36mDDGS._run_async_in_thread\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs an async coroutine in a separate thread.\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m future: Future[Any] \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m---> 44\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:158\u001b[0m, in \u001b[0;36mAsyncDDGS.text\u001b[1;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[0;32m    155\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml is not installed. Using backend=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 158\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_api(keywords, region, safesearch, timelimit, max_results)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    160\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_html(keywords, region, safesearch, timelimit, max_results)\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:192\u001b[0m, in \u001b[0;36mAsyncDDGS._text_api\u001b[1;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"DuckDuckGo text search generator. Query params: https://duckduckgo.com/params.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    TimeoutException: Inherits from DuckDuckGoSearchException, raised for API request timeouts.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m keywords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords is mandatory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 192\u001b[0m vqd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_vqd(keywords)\n\u001b[0;32m    194\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords,\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: region,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    203\u001b[0m }\n\u001b[0;32m    204\u001b[0m safesearch \u001b[38;5;241m=\u001b[39m safesearch\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\duckduckgo_search_async.py:121\u001b[0m, in \u001b[0;36mAsyncDDGS._aget_vqd\u001b[1;34m(self, keywords)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vqd value for a search query.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m resp_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://duckduckgo.com\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords})\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_extract_vqd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\duckduckgo_search\\utils.py:42\u001b[0m, in \u001b[0;36m_extract_vqd\u001b[1;34m(html_bytes, keywords)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m DuckDuckGoSearchException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_extract_vqd() \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m Could not extract vqd.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mDuckDuckGoSearchException\u001b[0m: _extract_vqd() keywords='SP!R!T INVEST 8900 Ieper' Could not extract vqd."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from duckduckgo_search import DDGS\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "# Rate limiting delay\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Adjust as per your rate limit settings\n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    delay_between_requests = DELAY_BETWEEN_REQUESTS\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}\"\n",
    "        response = requests.get(url, params=kwargs)\n",
    "        response.raise_for_status()  # Check for HTTP-level issues\n",
    "        data = response.json()  # Parse JSON response\n",
    "        return data\n",
    "    except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "        print(f\"Encountered a network error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, min_results=5, max_retries=1):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) == min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += 10  # Prepare to query next page of results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)  # Short delay before retrying\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    google_results = scrape_top_urls_google(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    all_results = {}\n",
    "    for url in set(google_results + ddg_results):\n",
    "        all_results[url] = {'ranks': [], 'appearances': 0}\n",
    "    \n",
    "    for rank, url in enumerate(google_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "    for rank, url in enumerate(ddg_results, start=1):\n",
    "        if url in all_results:\n",
    "            all_results[url]['ranks'].append(rank)\n",
    "            all_results[url]['appearances'] += 1\n",
    "\n",
    "    scored_urls = {}\n",
    "    for url, data in all_results.items():\n",
    "        mean_rank = sum(data['ranks']) / len(data['ranks']) if data['ranks'] else 100  # Penalize if not found\n",
    "        penalty = 0 if data['appearances'] == 2 else 10  # Penalty for appearance in one engine only\n",
    "        scored_urls[url] = mean_rank + penalty\n",
    "\n",
    "    sorted_urls = sorted(scored_urls.items(), key=lambda item: item[1])\n",
    "    top_urls_with_scores = [(url, round(score, 2)) for url, score in sorted_urls][:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "def save_last_processed_entity(entity_number):\n",
    "    with open('last_processed_entity.txt', 'w') as file:\n",
    "        file.write(str(entity_number))\n",
    "\n",
    "def get_last_processed_entity():\n",
    "    try:\n",
    "        with open('last_processed_entity.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def append_to_csv(new_row, file_name='search_results_Multi.csv'):\n",
    "    with open(file_name, 'a', newline='') as file:\n",
    "        new_row.to_csv(file, header=file.tell()==0, index=False)\n",
    "def main():\n",
    "    last_processed = get_last_processed_entity()\n",
    "    start_index = df.index[df['EntityNumber'] == last_processed].tolist()[0] + 1 if last_processed is not None else 0\n",
    "    \n",
    "    for index, row in df.iloc[start_index:].iterrows():\n",
    "        query = row['SearchQuery']\n",
    "        entity_number = row['EntityNumber']\n",
    "        \n",
    "        top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains=[], max_results=5)  # Ensure you have defined or included skip_domains list\n",
    "        top_urls = [url for url, score in top_urls_with_scores]\n",
    "\n",
    "        new_row = pd.DataFrame([{\n",
    "            \"EntityNumber\": entity_number, \n",
    "            \"URL1\": top_urls[0] if len(top_urls) > 0 else \"\", \n",
    "            \"URL2\": top_urls[1] if len(top_urls) > 1 else \"\", \n",
    "            \"URL3\": top_urls[2] if len(top_urls) > 2 else \"\", \n",
    "            \"URL4\": top_urls[3] if len(top_urls) > 3 else \"\", \n",
    "            \"URL5\": top_urls[4] if len(top_urls) > 4 else \"\"\n",
    "        }])\n",
    "        \n",
    "        append_to_csv(new_row)\n",
    "        save_last_processed_entity(entity_number)\n",
    "\n",
    "        progress_percentage = ((index + 1) / len(df)) * 100\n",
    "        print(f\"Progress: {progress_percentage:.2f}% ({index + 1}/{len(df)})\")\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
