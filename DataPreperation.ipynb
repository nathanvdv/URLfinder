{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                       OfficialName ZipCode  \\\n",
      "0  0201.310.929                                                IGL    3600   \n",
      "1  0202.239.951                                           PROXIMUS    1030   \n",
      "2  0203.201.340                          Nationale Bank van België    1000   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...    9100   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...    1210   \n",
      "\n",
      "          Municipality                Street HouseNumber  \\\n",
      "0                 Genk            Klotstraat         125   \n",
      "1           Schaarbeek  Koning AlbertII laan          27   \n",
      "2              Brussel     de Berlaimontlaan          14   \n",
      "3         Sint-Niklaas             Lamstraat         113   \n",
      "4  Sint-Joost-ten-Node           Galileelaan           5   \n",
      "\n",
      "                      URL  \n",
      "0  extranet.iglimburg.be/  \n",
      "1        www.proximus.com  \n",
      "2              www.nbb.be  \n",
      "3        www.interwaas.be  \n",
      "4       www.inami.fgov.be  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              SearchQuery\n",
      "0                                                                    Website IGL 3600 Genk Klotstraat 125\n",
      "1                                                Website PROXIMUS 1030 Schaarbeek Koning AlbertII laan 27\n",
      "2                                     Website Nationale Bank van België 1000 Brussel de Berlaimontlaan 14\n",
      "3    Website Intergemeentelijk Samenwerkingsverband van het Land van Waas 9100 Sint-Niklaas Lamstraat 113\n",
      "4  Website Rijksinstituut voor Ziekte- en Invaliditeitsverzekering 1210 Sint-Joost-ten-Node Galileelaan 5\n",
      "Website DATACTION 9320 Aalst Ninovesteenweg 198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"Website {row['OfficialName']} {row['ZipCode']} {row['Municipality']} {row['Street']} {row['HouseNumber']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())\n",
    "print(df.iloc[1600]['SearchQuery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs:\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/\n",
      "https://iglimburg.be/contact/\n",
      "https://app.akov.be/pls/pakov/f?p=INSP_PUBLIEK:VERSLAGEN::DOWNLOAD:::P1000_DLSEC_BLOB_ID:5596\n",
      "https://www.desocialekaart.be/fiches/7241847ec66a49801840a21ebb5c9eca8f8ac8b6bf162a15f978edbe96ab1789/administratieve-gegevens\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "# Assuming 'df' is your DataFrame loaded with the necessary data\n",
    "\n",
    "# Function to use Google's Custom Search JSON API\n",
    "def google_search(query, api_key, cse_id, **kwargs):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}\"\n",
    "    response = requests.get(url, params=kwargs)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def scrape_top_urls(search_query, search_engine, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    \n",
    "    if search_engine == \"Google\":\n",
    "        with open('config.json') as config_file:\n",
    "            config = json.load(config_file)\n",
    "            api_key = config['GOOGLE_API_KEY']\n",
    "            cse_id = config['GOOGLE_CSE_ID']\n",
    "            \n",
    "            \"\"\" Create a config.json file where you copy paste these lines: \n",
    "            {  \n",
    "            \"GOOGLE_API_KEY\": \"your_api_key_here\",\n",
    "            \"GOOGLE_CSE_ID\": \"your_cse_id_here\"\n",
    "            }\n",
    "            \"\"\"\n",
    "        \n",
    "        # Fetch more results initially to account for skipped URLs\n",
    "        results = google_search(search_query, api_key, cse_id, num=max_results)\n",
    "        \n",
    "        # Filter out URLs from domains you want to skip and keep collecting until you have 5 (or run out)\n",
    "        for item in results.get('items', []):\n",
    "            url = item['link']\n",
    "            # Check if URL is from a domain to skip\n",
    "            if not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Stop once you have 5 URLs after filtering\n",
    "                if len(top_urls) == 5:\n",
    "                    break\n",
    "            \n",
    "    return top_urls\n",
    "\n",
    "# List of domains to skip\n",
    "skip_domains = ['trendstop.knack.b', 'fincheck.be', 'bizzy.org', 'trendstop.levif.be']\n",
    "\n",
    "# Fetch URLs from Google, skipping the specified domains\n",
    "# Example: Using the first search query in your DataFrame for demonstration\n",
    "first_search_query = df.iloc[0]['SearchQuery']\n",
    "filtered_urls = scrape_top_urls(first_search_query, \"Google\", skip_domains)\n",
    "\n",
    "# Print the filtered URLs\n",
    "print(\"Filtered URLs:\")\n",
    "for url in filtered_urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered URLs:\n",
      "https://iglimburg.be/contact/\n",
      "https://www.linkedin.com/company/iglimburg\n",
      "https://iglimburg.be/\n",
      "https://www.terheide.be/\n",
      "https://opencorporates.com/companies/be/0201310929\n"
     ]
    }
   ],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions.\n",
    "#%pip install duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=5):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "    \n",
    "    for result in results:\n",
    "        url = result.get('href')\n",
    "        if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "            top_urls.append(url)\n",
    "            if len(top_urls) == max_results:\n",
    "                break\n",
    "\n",
    "    return top_urls\n",
    "\n",
    "first_search_query = df.iloc[0]['SearchQuery']\n",
    "filtered_urls = scrape_top_urls_ddg(first_search_query, skip_domains, 5)\n",
    "\n",
    "print(\"Filtered URLs:\")\n",
    "for url in filtered_urls:\n",
    "    print(url)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
