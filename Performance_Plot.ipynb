{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# For DuckDuckGo\n",
    "data_duckduckgo = {\n",
    "    'Model': ['LR', 'DT', 'RF', 'GB', 'SVM', 'NN', 'K-NN'],\n",
    "    'F1-Score (macro avg)': [0.77, 0.70, 0.79, 0.79, 0.78, 0.75, 0.75],\n",
    "    'Hamming Loss': [0.0834, 0.118, 0.076, 0.0767, 0.078, 0.096, 0.089],\n",
    "    'Jaccard Score': [0.8058, 0.690, 0.820, 0.82, 0.819, 0.758, 0.798]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_duckduckgo = pd.DataFrame(data_duckduckgo)\n",
    "\n",
    "# For DuckDuckGo_2\n",
    "data_duckduckgo_2 = {\n",
    "    'Model': ['LR', 'DT', 'RF', 'GB', 'SVM', 'NN', 'K-NN'],\n",
    "    'F1-Score (macro avg)': [0.78, 0.68, 0.79, 0.80, 0.79, 0.75, 0.75],\n",
    "    'Hamming Loss': [0.107, 0.154, 0.099, 0.098, 0.101, 0.121, 0.119],\n",
    "    'Jaccard Score': [0.769, 0.628, 0.776, 0.788, 0.78, 0.716, 0.752]\n",
    "}\n",
    "df_duckduckgo_2 = pd.DataFrame(data_duckduckgo_2)\n",
    "\n",
    "# For Google\n",
    "data_google = {\n",
    "    'Model': ['LR', 'DT', 'RF', 'GB', 'SVM', 'NN', 'K-NN'],\n",
    "    'F1-Score (macro avg)': [0.82, 0.73, 0.84, 0.83, 0.84, 0.79, 0.80],\n",
    "    'Hamming Loss': [0.075, 0.112, 0.068, 0.071, 0.069, 0.089, 0.080],\n",
    "    'Jaccard Score': [0.809, 0.695, 0.827, 0.828, 0.826, 0.769, 0.809]\n",
    "}\n",
    "df_google = pd.DataFrame(data_google)\n",
    "\n",
    "# For Google_2\n",
    "data_google_2 = {\n",
    "    'Model': ['LR', 'DT', 'RF', 'GB', 'SVM', 'NN', 'K-NN'],\n",
    "    'F1-Score (macro avg)': [0.83, 0.73, 0.84, 0.84, 0.83, 0.80, 0.79],\n",
    "    'Hamming Loss': [0.065, 0.101, 0.059, 0.061, 0.061, 0.075, 0.075],\n",
    "    'Jaccard Score': [0.848, 0.736, 0.856, 0.855, 0.855, 0.810, 0.834]\n",
    "}\n",
    "df_google_2 = pd.DataFrame(data_google_2)\n",
    "\n",
    "# For Multi Dataset\n",
    "data_multi = {\n",
    "    'Model': ['LR', 'DT', 'RF', 'GB', 'SVM', 'NN', 'K-NN'],\n",
    "    'F1-Score (macro avg)': [0.81, 0.72, 0.83, 0.83, 0.82, 0.77, 0.79],\n",
    "    'Hamming Loss': [0.101, 0.151, 0.092, 0.093, 0.095, 0.117, 0.113],\n",
    "    'Jaccard Score': [0.768, 0.616, 0.781, 0.787, 0.779, 0.716, 0.751]\n",
    "}\n",
    "df_multi = pd.DataFrame(data_multi)\n",
    "\n",
    "\n",
    "print(df_duckduckgo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalize metrics\n",
    "def normalize_metrics(df, metrics_to_maximize, metrics_to_minimize):\n",
    "    # Normalize metrics where higher is better\n",
    "    for metric in metrics_to_maximize:\n",
    "        df[metric] = (df[metric] - df[metric].min()) / (df[metric].max() - df[metric].min())\n",
    "    \n",
    "    # Normalize metrics where lower is better\n",
    "    for metric in metrics_to_minimize:\n",
    "        df[metric] = (df[metric].max() - df[metric]) / (df[metric].max() - df[metric].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to plot metrics comparison for all models\n",
    "def plot_metrics_comparison_all(df, dataset_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = df.columns[1:]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, len(metrics), i+1)\n",
    "        bars = plt.barh(df['Model'], df[metric], color='skyblue')\n",
    "        plt.xlabel(metric)\n",
    "        plt.title(f'{metric} Comparison for {dataset_name}')\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width / 2, bar.get_y() + bar.get_height()/2, f'{width:.2f}', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name}_metrics_comparison_all.jpg')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot metrics comparison for top 3 models\n",
    "def plot_metrics_comparison_top3(df, dataset_name):\n",
    "    df_normalized = normalize_metrics(df.copy(), ['F1-Score (macro avg)', 'Jaccard Score'], ['Hamming Loss'])\n",
    "    df_top_models = df_normalized.nlargest(3, 'F1-Score (macro avg)')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = df.columns[1:]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, len(metrics), i+1)\n",
    "        bars = plt.bar(df_top_models['Model'], df_top_models[metric], color='salmon')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'Top 3 Models Comparison for {dataset_name}')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height/2, f'{height:.2f}', ha='center', va='center', color='white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{dataset_name}_top_models_comparison.jpg')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting metrics comparison for all models\n",
    "plot_metrics_comparison_all(df_multi, 'Multi')\n",
    "\n",
    "# Plotting metrics comparison for top 3 models\n",
    "plot_metrics_comparison_top3(df_multi, 'Multi')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
