{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the .parquet file of the combined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"combined_filtered_dataset.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(row):\n",
    "    \"\"\"\n",
    "    Generates a search query string for a given row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A pandas Series representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the search query.\n",
    "    \"\"\"\n",
    "    # Construct the query using the business information\n",
    "    query = f\"{row['OfficialName']} {row['ZipCode']} {row['Municipality']}\"\n",
    "    return query\n",
    "\n",
    "# Apply the function to each row in the DataFrame to create the queries\n",
    "df['SearchQuery'] = df.apply(generate_query, axis=1)\n",
    "\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns are displayed\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full content of each cell is displayed\n",
    "pd.set_option('display.width', None)  # Adjust the display width for readability\n",
    "\n",
    "# Show the DataFrame with the generated queries\n",
    "print(df[['SearchQuery']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset_incl_query.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping of search engines results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = ['trendstop.knack.be', 'fincheck.be', 'bizzy.org'\n",
    "                , 'trendstop.levif.be', 'companyweb.be', 'linkedin.com'\n",
    "                , 'en.wikipedia.org', 'facebook.com', 'be.linkedin.com'\n",
    "                , 'instagram.com', 'werkenbijdeoverheid.be', 'dnb.com', 'nl.wikipedia.org'\n",
    "                , 'youtube.com', 'staatsbladmonitor.be', 'werkenvoor.be'\n",
    "                , 'twitter.com', 'vlaanderen.be/organisaties', 'jobat.be'\n",
    "                , 'vdab.be', 'opencorporates.com','www.goldenpages.be',\n",
    "                'www.immoweb.be', 'be.kompass.com','www.infobel.com',\n",
    "                'www.bsearch.be', 'www.creditsafe.com','openthebox.be',\n",
    "                'bedrijvengids.cybo.com','data.be','www.yelp.com',\n",
    "                'www.goudengids.be','gb.kompass.com','www.cylex-belgie.be',\n",
    "                'local.infobel.be','www.cybo.com','www.viamichelin.com','lokaal.infobel.be',\n",
    "                'www.northdata.com','www.tripadvisor.com','www.zoominfo.com',\n",
    "                'fr.kompass.com','www.openingsuren.vlaanderen','www.info-clipper.com',\n",
    "                'www.northdata.de','b2bhint.com','www.realo.be',\n",
    "                'www.pagesdor.be','www.worldpostalcodes.org','www.openingsurengids.be',\n",
    "                'open-winkel.be','opencorpdata.com','lemariagedelouise.be',\n",
    "                'www.signalhire.com','www.faillissementsdossier.be','www.bizique.be',\n",
    "                'www.booking.com','www.hours.be','www.handelsgids.be',\n",
    "                'foursquare.com','zaubee.com','be.top10place.com',\n",
    "                'restaurantguru.com','www.zimmo.be','guide.michelin.com',\n",
    "                'selfcity.be','belgium.worldplaces.me','www.boekhoudkantoren.be',\n",
    "                'jaarrekening.be']\n",
    "#Sets are faster\n",
    "skip_domains = set(skip_domains)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config_search.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    base_delay = 0.1  # Base delay for exponential backoff\n",
    "    max_delay = 20  # Maximum delay between retries\n",
    "    attempt = 0  # Initial attempt number\n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}&cr=countryBE&lr=lang_nl\"\n",
    "            response = requests.get(url, params=kwargs)\n",
    "            response.raise_for_status()  # Check for HTTP-level issues\n",
    "            data = response.json()  # Parse JSON response\n",
    "            return data\n",
    "        except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "            print(f\"Encountered a network error: {e}. Retrying...\")\n",
    "            attempt += 1\n",
    "            delay = min(max_delay, base_delay * 2 ** attempt)  # Exponential backoff\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains_set, min_results=5, max_retries=3):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "    skip_domains_set = set(skip_domains)  # Convert list to set for efficient testing\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains_set) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) >= min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += len(results.get('items', []))  # Adjust based on actual returned results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(1)  # Longer delay before retrying could be beneficial here\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = 'search_results_Google_2.csv'\n",
    "    placeholder = \"\"  # Placeholder for unfound URLs\n",
    "    batch_size = 20  # Number of queries to process before saving to CSV\n",
    "    processed_batch = []  # Initialize the batch list\n",
    "\n",
    "    try:\n",
    "        progress_df = pd.read_csv(file_path)\n",
    "        last_processed_entity = progress_df['EntityNumber'].max() if not progress_df.empty else None\n",
    "    except FileNotFoundError:\n",
    "        progress_df = pd.DataFrame(columns=['EntityNumber', 'URL1', 'URL2', 'URL3', 'URL4', 'URL5'])\n",
    "        last_processed_entity = None\n",
    "\n",
    "    total_queries = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        entity_number = row['EntityNumber']\n",
    "        if last_processed_entity and entity_number <= last_processed_entity:\n",
    "            continue\n",
    "\n",
    "        # Perform search and process results\n",
    "        filtered_urls = scrape_top_urls_google(row['SearchQuery'], skip_domains, 5)\n",
    "        urls_to_add = [filtered_urls[i] if i < len(filtered_urls) else placeholder for i in range(5)]\n",
    "        \n",
    "        processed_batch.append({\"EntityNumber\": entity_number, \"URL1\": urls_to_add[0], \"URL2\": urls_to_add[1], \"URL3\": urls_to_add[2], \"URL4\": urls_to_add[3], \"URL5\": urls_to_add[4]})\n",
    "\n",
    "        if len(processed_batch) >= batch_size or index == total_queries - 1:\n",
    "            # Append batch to DataFrame and reset for next batch\n",
    "            new_rows_df = pd.DataFrame(processed_batch)\n",
    "            progress_df = pd.concat([progress_df, new_rows_df], ignore_index=True)\n",
    "            progress_df.to_csv(file_path, index=False)\n",
    "            processed_batch = []  # Clear the batch\n",
    "\n",
    "        # Update progress\n",
    "        processed_entries = index + 1 \n",
    "        percentage_completed = (processed_entries / total_queries) * 100\n",
    "        print(f\"Processed EntityNumber {entity_number}. Completion: {percentage_completed:.2f}% [{processed_entries}/{total_queries}]\")\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo\n",
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you'd install the package, usually via pip. Check the repository for the latest instructions. \n",
    "#%pip install -U duckduckgo_search\n",
    "# Uses https://github.com/deedy5/duckduckgo_search\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "try:\n",
    "    result_df = pd.read_csv('search_results_DDG.csv')\n",
    "    collected_data = result_df.to_dict('records')\n",
    "except FileNotFoundError:\n",
    "    collected_data = []\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if any(d['EntityNumber'] == row['EntityNumber'] for d in collected_data):\n",
    "        continue  # Skip already processed\n",
    "    \n",
    "    search_query = row['SearchQuery']\n",
    "    entity_number = row['EntityNumber']\n",
    "    try:\n",
    "        filtered_urls = scrape_top_urls_ddg(search_query, skip_domains, max_results=5)\n",
    "        time.sleep(1)  # Enforce a simple rate limit\n",
    "\n",
    "        collected_data.append({\n",
    "            \"EntityNumber\": entity_number, \n",
    "            \"URL1\": filtered_urls[0] if len(filtered_urls) > 0 else \"\", \n",
    "            \"URL2\": filtered_urls[1] if len(filtered_urls) > 1 else \"\", \n",
    "            \"URL3\": filtered_urls[2] if len(filtered_urls) > 2 else \"\", \n",
    "            \"URL4\": filtered_urls[3] if len(filtered_urls) > 3 else \"\", \n",
    "            \"URL5\": filtered_urls[4] if len(filtered_urls) > 4 else \"\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered: {e}. Waiting before retrying...\")\n",
    "        time.sleep(20)  \n",
    "        continue  \n",
    "\n",
    "    result_df = pd.DataFrame(collected_data)\n",
    "    result_df.to_csv('search_results_DDG.csv', index=False)\n",
    "\n",
    "    processed_entries = index + 1  \n",
    "    percentage_completed = (processed_entries / total_rows) * 100\n",
    "    print(f\"Processed EntityNumber {entity_number}. Completion: {percentage_completed:.2f}% [{processed_entries}/{total_rows}]\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U duckduckgo_search\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from duckduckgo_search import DDGS\n",
    "from requests.exceptions import ConnectionError, HTTPError\n",
    "from urllib3.exceptions import ProtocolError, NewConnectionError\n",
    "\n",
    "# Load API key and CSE ID from config\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "api_key = config['GOOGLE_API_KEY']\n",
    "cse_id = config['GOOGLE_CSE_ID']\n",
    "\n",
    "# Rate limiting delay\n",
    "DELAY_BETWEEN_REQUESTS = 0.5 \n",
    "\n",
    "def google_search(query, api_key, cse_id, start=1, **kwargs):\n",
    "    delay_between_requests = DELAY_BETWEEN_REQUESTS\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&start={start}\"\n",
    "        response = requests.get(url, params=kwargs)\n",
    "        response.raise_for_status()  # Check for HTTP-level issues\n",
    "        data = response.json()  # Parse JSON response\n",
    "        return data\n",
    "    except (ConnectionError, ProtocolError, HTTPError) as e:\n",
    "        print(f\"Encountered a network error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_top_urls_google(search_query, skip_domains, min_results=5, max_retries=1):\n",
    "    top_urls = []  # Initialize as an empty list\n",
    "    retries = 0\n",
    "    current_start = 1\n",
    "\n",
    "    while len(top_urls) < min_results and retries < max_retries:\n",
    "        results = google_search(search_query, api_key, cse_id, start=current_start, num=10)\n",
    "        if results and 'items' in results:\n",
    "            for item in results['items']:\n",
    "                url = item['link']\n",
    "                if not any(skip_domain in url for skip_domain in skip_domains) and url not in top_urls:\n",
    "                    top_urls.append(url)\n",
    "                if len(top_urls) == min_results:\n",
    "                    break  # Found enough URLs, exit loop\n",
    "            current_start += 10  # Prepare to query next page of results\n",
    "        else:\n",
    "            print(f\"No results found in attempt {retries + 1}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)  # Short delay before retrying\n",
    "\n",
    "    return top_urls[:min_results]\n",
    "\n",
    "def scrape_top_urls_ddg(search_query, skip_domains, max_results=10):\n",
    "    top_urls = []\n",
    "    ddgs = DDGS()\n",
    "    try:\n",
    "        # Fetch results with potentially more than needed to account for skipped domains\n",
    "        results = ddgs.text(keywords=search_query, max_results=max_results + len(skip_domains))\n",
    "        \n",
    "        for result in results:\n",
    "            url = result.get('href')\n",
    "            # Check if URL should be skipped\n",
    "            if url and not any(skip_domain in url for skip_domain in skip_domains):\n",
    "                top_urls.append(url)\n",
    "                # Break if enough URLs have been collected\n",
    "                if len(top_urls) == max_results:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered an error: {e}\")\n",
    "        raise  # Re-raise the exception to handle it outside\n",
    "    \n",
    "    return top_urls\n",
    "\n",
    "def perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5):\n",
    "    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "    google_results = scrape_top_urls_google(query, skip_domains, max_results)\n",
    "    ddg_results = scrape_top_urls_ddg(query, skip_domains, max_results)\n",
    "\n",
    "    combined_scores = calculate_combined_rank_score(google_results, ddg_results)\n",
    "\n",
    "    top_urls_with_scores = combined_scores[:max_results]\n",
    "\n",
    "    return top_urls_with_scores\n",
    "\n",
    "def calculate_combined_rank_score(google_results, ddg_results):\n",
    "    combined_scores = {}\n",
    "    max_rank = max(len(google_results), len(ddg_results)) + 1\n",
    "\n",
    "    all_urls = set(google_results + ddg_results)\n",
    "    for url in all_urls:\n",
    "        google_rank = google_results.index(url) + 1 if url in google_results else max_rank\n",
    "        ddg_rank = ddg_results.index(url) + 1 if url in ddg_results else max_rank\n",
    "        combined_score = google_rank * ddg_rank\n",
    "        combined_scores[url] = combined_score\n",
    "\n",
    "    sorted_urls_with_scores = sorted(combined_scores.items(), key=lambda item: item[1])\n",
    "    final_scores = [(url, 1.0 / combined_score) for url, combined_score in sorted_urls_with_scores]\n",
    "    return final_scores\n",
    "\n",
    "\n",
    "def save_last_processed_entity(entity_number):\n",
    "    with open('last_processed_entity.txt', 'w') as file:\n",
    "        file.write(str(entity_number))\n",
    "\n",
    "def get_last_processed_entity():\n",
    "    try:\n",
    "        with open('last_processed_entity.txt', 'r') as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def find_missing_entities(df, csv_path='search_results_Multi.csv'):\n",
    "    \"\"\"Find entity numbers in df that are missing in the CSV.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return set(df['EntityNumber'])  # If CSV doesn't exist, all entities are missing\n",
    "\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    existing_entities = set(existing_df['EntityNumber'])\n",
    "    all_entities = set(df['EntityNumber'])\n",
    "    \n",
    "    missing_entities = all_entities - existing_entities\n",
    "    return missing_entities\n",
    "\n",
    "def process_entity(entity_number, query, skip_domains):\n",
    "    \"\"\"Process a single entity number.\"\"\"\n",
    "    top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5)\n",
    "    top_urls = [url for url, score in top_urls_with_scores]\n",
    "\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"EntityNumber\": entity_number, \n",
    "        \"URL1\": top_urls[0] if len(top_urls) > 0 else \"\", \n",
    "        \"URL2\": top_urls[1] if len(top_urls) > 1 else \"\", \n",
    "        \"URL3\": top_urls[2] if len(top_urls) > 2 else \"\", \n",
    "        \"URL4\": top_urls[3] if len(top_urls) > 3 else \"\", \n",
    "        \"URL5\": top_urls[4] if len(top_urls) > 4 else \"\"\n",
    "    }])\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "def append_to_csv(new_row, file_name='search_results_Multi.csv'):\n",
    "    with open(file_name, 'a', newline='', encoding='utf-8') as file:\n",
    "        new_row.to_csv(file, header=file.tell()==0, index=False)\n",
    "        \n",
    "def main():\n",
    "    scores_df = pd.DataFrame(columns=['EntityNumber', 'URL', 'Score'])\n",
    "    missing_entities = find_missing_entities(df)\n",
    "    \n",
    "    for entity_number in missing_entities:\n",
    "        row = df[df['EntityNumber'] == entity_number].iloc[0] \n",
    "        query = row['SearchQuery']\n",
    "        \n",
    "        # Process each entity\n",
    "        new_row = process_entity(entity_number, query, skip_domains)\n",
    "        append_to_csv(new_row)\n",
    "        save_last_processed_entity(entity_number)\n",
    "        \n",
    "        # Perform search to get URLs and scores\n",
    "        top_urls_with_scores = perform_multi_search_with_scores_and_penalty(query, skip_domains, max_results=5)\n",
    "        \n",
    "        # After collecting URLs and scores, append them to the scores data frame\n",
    "        for url, score in top_urls_with_scores:\n",
    "            new_score_row = pd.DataFrame({'EntityNumber': [entity_number], 'URL': [url], 'Score': [score]})\n",
    "            scores_df = pd.concat([scores_df, new_score_row], ignore_index=True)\n",
    "        \n",
    "        print(f\"Processed entity number: {entity_number}\")\n",
    "    \n",
    "    # Optionally save scores_df to a file or handle it as needed\n",
    "    scores_df.to_csv('final_scores.csv', index=False)\n",
    "\n",
    "    print(\"All data has been processed and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if all entity number are present and rearagne the entities in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_entity_numbers_in_csv(df, csv_path):\n",
    "    \"\"\"\n",
    "    Check if all entity numbers in the dataframe (df) are present in the CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing entity numbers.\n",
    "    - csv_path: The path to the CSV file to check against.\n",
    "    \n",
    "    Returns:\n",
    "    - (bool, list): A tuple containing a boolean indicating if all entities are present, and a list of missing entities if any.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV file does not exist.\")\n",
    "        return False, list(df['EntityNumber'])  # If CSV doesn't exist, all entities are missing\n",
    "\n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "    missing_entities = set(df['EntityNumber']) - set(csv_df['EntityNumber'])\n",
    "    \n",
    "    return len(missing_entities) == 0, list(missing_entities)\n",
    "\n",
    "\n",
    "def reorder_csv_to_match_df(df, csv_path):\n",
    "    \"\"\"\n",
    "    Reorder the entries in the CSV file to match the order of entries in the DataFrame (df) and overwrite the original CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the desired order of entity numbers.\n",
    "    - csv_path: The path to the CSV file to reorder and overwrite.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"CSV file does not exist.\")\n",
    "        return\n",
    "    \n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Reorder the csv_df to match the order in df\n",
    "    reordered_df = pd.merge(df[['EntityNumber']], csv_df, on='EntityNumber', how='left')\n",
    "    \n",
    "    # Overwrite the original CSV file with the reordered DataFrame\n",
    "    reordered_df.to_csv(csv_path, index=False)\n",
    "    print(f\"CSV has been reordered and saved to {csv_path}.\")\n",
    "\n",
    "# Use of functions\n",
    "all_present, missing_entities = check_entity_numbers_in_csv(df, 'search_results_Multi.csv')\n",
    "if all_present:\n",
    "    print(\"All entity numbers are present in the CSV.\")\n",
    "else:\n",
    "    print(f\"Missing entity numbers: {missing_entities}\")\n",
    "    print(f\"Are missing: ({len(missing_entities)})\")\n",
    "    \n",
    "#Only execute if the dataset is full\n",
    "#reorder_csv_to_match_df(df, 'search_results_Multi.csv') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
