{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BERT for URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.3.0)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 11)) (1.4.1.post1)\n",
      "Requirement already satisfied: keras in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 12)) (3.2.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 13)) (2.16.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 14)) (4.39.3)\n",
      "Requirement already satisfied: torch in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 15)) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 16)) (1.26.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 17)) (0.15.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 18)) (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 19)) (0.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 20)) (4.66.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: orjson>=3.10.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: optree in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow->-r requirements.txt (line 13)) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (2.16.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (0.4.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tqdm->-r requirements.txt (line 20)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 15)) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from rich->keras->-r requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from rich->keras->-r requirements.txt (line 12)) (2.17.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.41.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/dataset_incl_query.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m domain\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m dataset_query \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/dataset_incl_query.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m dataset_scraped \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/search_results_DDG.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Merge datasets on 'EntityNumber'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset_incl_query.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to preprocess and extract domain from URLs\n",
    "def extract_domain(url):\n",
    "    # Check if the URL is not a string (e.g., NaN or None)\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"  # Return an empty string to indicate no domain\n",
    "        # Clean the URL by removing slashes and quotation marks\n",
    "    \n",
    "    # Extract the domain\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc or parsed_url.path  # Fallback to path if netloc is empty (e.g., relative URLs)\n",
    "    domain = domain.replace('www.', '')  # Removing 'www.' for consistency\n",
    "    domain = domain.replace('/', '').replace('\"', '')\n",
    "    return domain\n",
    "\n",
    "# Load the datasets\n",
    "dataset_query = pd.read_csv('dataset_incl_query.csv')\n",
    "dataset_scraped = pd.read_csv('search_results_DDG.csv')\n",
    "\n",
    "# Merge datasets on 'EntityNumber'\n",
    "merged_dataset = pd.merge(dataset_query[['EntityNumber', 'URL', 'SearchQuery']], dataset_scraped, on='EntityNumber')\n",
    "\n",
    "# Preprocess URLs to extract domains\n",
    "merged_dataset['CorrectDomain'] = merged_dataset['URL'].apply(extract_domain)\n",
    "for i in range(1, 6):\n",
    "    merged_dataset[f'URL{i}Domain'] = merged_dataset[f'URL{i}'].apply(extract_domain)\n",
    "\n",
    "# Prepare labels: If the correct domain matches one of the scraped domains, label with that index; otherwise, label as -1\n",
    "# Adjust the labeling function to handle multiple correct URLs\n",
    "def mark_correct_labels(row):\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        # Check if each scraped domain matches the correct domain\n",
    "        if row['CorrectDomain'] == row[f'URL{i}Domain']:\n",
    "            labels.append(1)  # Mark as correct\n",
    "        else:\n",
    "            labels.append(0)  # Mark as incorrect\n",
    "    return labels\n",
    "\n",
    "# Apply the function to each row in the merged dataset\n",
    "merged_dataset['Labels'] = merged_dataset.apply(mark_correct_labels, axis=1)\n",
    "\n",
    "# Display the updated dataset with domains and new labels for inspection\n",
    "print(merged_dataset[['EntityNumber', 'SearchQuery', 'CorrectDomain', 'URL1Domain', 'URL2Domain', 'URL3Domain', 'URL4Domain', 'URL5Domain', 'Labels']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merged_dataset.drop(['URL', 'URL1', 'URL2', 'URL3', 'URL4', 'URL5'], axis=1)\n",
    "print(merged_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepreration for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class URLDomainDataset(Dataset):\n",
    "    def __init__(self, queries, domains, labels, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.queries = queries\n",
    "        self.domains = domains  # List of lists containing domains for each query\n",
    "        self.labels = labels    # List of lists containing binary labels for each domain\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)  # Ensure correct shape [5]\n",
    "        \n",
    "        # Assuming you are combining query with each domain\n",
    "        # This simplifies the example; adapt as necessary for your actual tokenization logic\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),  # Shape [max_len]\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),  # Shape [max_len]\n",
    "            'labels': labels  # Shape [5]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Prepare data for splitting\n",
    "X = merged_dataset[['SearchQuery', 'URL1Domain', 'URL2Domain', 'URL3Domain', 'URL4Domain', 'URL5Domain']].values\n",
    "y = np.array(merged_dataset['Labels'].tolist())  # Convert labels to a suitable format\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data\n",
    "queries = merged_dataset['SearchQuery'].tolist()\n",
    "domains = merged_dataset[[f'URL{i}Domain' for i in range(1, 6)]].values.tolist()\n",
    "labels = merged_dataset['Labels'].tolist()\n",
    "# Prepare the data for the Dataset instances\n",
    "train_queries = [x[0] for x in X_train]  # Assuming the first column of X is 'SearchQuery'\n",
    "train_domains = [x[1:] for x in X_train]  # Assuming the rest are 'URL1Domain' to 'URL5Domain'\n",
    "train_labels = y_train\n",
    "\n",
    "val_queries = [x[0] for x in X_val]  # Same assumption for validation data\n",
    "val_domains = [x[1:] for x in X_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = URLDomainDataset(train_queries, train_domains, train_labels, tokenizer)\n",
    "val_dataset = URLDomainDataset(val_queries, val_domains, val_labels, tokenizer)\n",
    "\n",
    "# Initializing Dataset\n",
    "dataset = URLDomainDataset(queries, domains, labels, tokenizer)\n",
    "\n",
    "# Initializing DataLoader\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Define an appropriate batch size for your model and hardware\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "\n",
    "# Assuming you're using a GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=5,  # Assuming you have 5 URL domains to classify per query\n",
    "    problem_type=\"multi_label_classification\",  # Specify the problem type\n",
    ").to(device)\n",
    "\n",
    "# Initialize the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)  # Adjust parameters as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "def evaluate_model_extended(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).round().cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hammingLoss = hamming_loss(true_labels, predictions)\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    precision_micro = precision_score(true_labels, predictions, average='micro')\n",
    "    recall_micro = recall_score(true_labels, predictions, average='micro')\n",
    "    \n",
    "    return hammingLoss, f1_micro, precision_micro, recall_micro\n",
    "\n",
    "    \n",
    "best_hamming_loss = float('inf')\n",
    "model_save_path = 'best_model_state.bin'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_func(outputs.logits, batch['labels'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    scheduler.step()  # Update the learning rate\n",
    "\n",
    "    # Evaluation step\n",
    "    hamming_loss_val, f1_micro, precision_micro, recall_micro = evaluate_model_extended(model, val_loader, device)\n",
    "    print(f'\\nValidation Metrics after Epoch {epoch + 1}:')\n",
    "    print(f'Hamming Loss: {hamming_loss_val}')\n",
    "    print(f'F1-Score (Micro): {f1_micro}')\n",
    "    print(f'Precision (Micro): {precision_micro}')\n",
    "    print(f'Recall (Micro): {recall_micro}')\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if hamming_loss_val < best_hamming_loss:\n",
    "        best_hamming_loss = hamming_loss_val\n",
    "        print(\"Hamming Loss improved, saving model...\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./BERT_model/model\")\n",
    "tokenizer.save_pretrained(\"./BERT_model/tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
