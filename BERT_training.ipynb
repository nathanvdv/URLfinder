{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning BERT for URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium>=3.141.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 1)) (4.16.0)\n",
      "Requirement already satisfied: webdriver-manager>=3.4.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 2)) (4.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 7)) (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: duckduckgo_search in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 9)) (5.3.0)\n",
      "Requirement already satisfied: ratelimit in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 11)) (1.4.1.post1)\n",
      "Requirement already satisfied: keras in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 12)) (3.2.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 13)) (2.16.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 14)) (4.39.3)\n",
      "Requirement already satisfied: torch in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 15)) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 16)) (1.26.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 17)) (0.15.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 18)) (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 19)) (0.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from -r requirements.txt (line 20)) (4.66.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from selenium>=3.141.0->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from webdriver-manager>=3.4.2->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: curl-cffi>=0.6.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (0.6.2)\n",
      "Requirement already satisfied: orjson>=3.10.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from duckduckgo_search->-r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: optree in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from keras->-r requirements.txt (line 12)) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow->-r requirements.txt (line 13)) (2.16.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (2.16.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from transformers->-r requirements.txt (line 14)) (0.4.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from torch->-r requirements.txt (line 15)) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tqdm->-r requirements.txt (line 20)) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio~=0.17->selenium>=3.141.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=3.141.0->-r requirements.txt (line 1)) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 15)) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from rich->keras->-r requirements.txt (line 12)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from rich->keras->-r requirements.txt (line 12)) (2.17.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.41.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo_search->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 13)) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\natha\\anaconda3\\envs\\urlfinder\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=3.141.0->-r requirements.txt (line 1)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                        SearchQuery  \\\n",
      "0  0201.310.929                                      IGL 3600 Genk   \n",
      "1  0202.239.951                           PROXIMUS 1030 Schaarbeek   \n",
      "2  0203.201.340             Nationale Bank van België 1000 Brussel   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...   \n",
      "\n",
      "           CorrectDomain             URL1Domain      URL2Domain  \\\n",
      "0  extranet.iglimburg.be           iglimburg.be  intergalva.com   \n",
      "1           proximus.com           proximus.com     proximus.be   \n",
      "2                 nbb.be                 nbb.be          nbb.be   \n",
      "3           interwaas.be  erfgoedcelwaasland.be         vvsg.be   \n",
      "4          inami.fgov.be          riziv.fgov.be   riziv.fgov.be   \n",
      "\n",
      "      URL3Domain               URL4Domain         URL5Domain           Labels  \n",
      "0   mapcarta.com       roamtechnology.com         geruro.com  [0, 0, 0, 0, 0]  \n",
      "1    proximus.be             proximus.com        proximus.be  [1, 0, 0, 1, 0]  \n",
      "2   nbbmuseum.be  openingsuren.vlaanderen       nbbmuseum.be  [1, 1, 0, 0, 0]  \n",
      "3    govserv.org           creditsafe.com            fsma.be  [0, 0, 0, 0, 0]  \n",
      "4  riziv.fgov.be         sociaal.brussels  desocialekaart.be  [0, 0, 0, 0, 0]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to preprocess and extract domain from URLs\n",
    "def extract_domain(url):\n",
    "    # Check if the URL is not a string (e.g., NaN or None)\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"  # Return an empty string to indicate no domain\n",
    "        # Clean the URL by removing slashes and quotation marks\n",
    "    \n",
    "    # Extract the domain\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc or parsed_url.path  # Fallback to path if netloc is empty (e.g., relative URLs)\n",
    "    domain = domain.replace('www.', '')  # Removing 'www.' for consistency\n",
    "    domain = domain.replace('/', '').replace('\"', '')\n",
    "    return domain\n",
    "\n",
    "# Load the datasets\n",
    "dataset_query = pd.read_csv('dataset_incl_query.csv')\n",
    "dataset_scraped = pd.read_csv('search_results_DDG.csv')\n",
    "\n",
    "# Merge datasets on 'EntityNumber'\n",
    "merged_dataset = pd.merge(dataset_query[['EntityNumber', 'URL', 'SearchQuery']], dataset_scraped, on='EntityNumber')\n",
    "\n",
    "# Preprocess URLs to extract domains\n",
    "merged_dataset['CorrectDomain'] = merged_dataset['URL'].apply(extract_domain)\n",
    "for i in range(1, 6):\n",
    "    merged_dataset[f'URL{i}Domain'] = merged_dataset[f'URL{i}'].apply(extract_domain)\n",
    "\n",
    "# Prepare labels: If the correct domain matches one of the scraped domains, label with that index; otherwise, label as -1\n",
    "# Adjust the labeling function to handle multiple correct URLs\n",
    "def mark_correct_labels(row):\n",
    "    labels = []\n",
    "    for i in range(1, 6):\n",
    "        # Check if each scraped domain matches the correct domain\n",
    "        if row['CorrectDomain'] == row[f'URL{i}Domain']:\n",
    "            labels.append(1)  # Mark as correct\n",
    "        else:\n",
    "            labels.append(0)  # Mark as incorrect\n",
    "    return labels\n",
    "\n",
    "# Apply the function to each row in the merged dataset\n",
    "merged_dataset['Labels'] = merged_dataset.apply(mark_correct_labels, axis=1)\n",
    "\n",
    "# Display the updated dataset with domains and new labels for inspection\n",
    "print(merged_dataset[['EntityNumber', 'SearchQuery', 'CorrectDomain', 'URL1Domain', 'URL2Domain', 'URL3Domain', 'URL4Domain', 'URL5Domain', 'Labels']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EntityNumber                                        SearchQuery  \\\n",
      "0  0201.310.929                                      IGL 3600 Genk   \n",
      "1  0202.239.951                           PROXIMUS 1030 Schaarbeek   \n",
      "2  0203.201.340             Nationale Bank van België 1000 Brussel   \n",
      "3  0206.460.639  Intergemeentelijk Samenwerkingsverband van het...   \n",
      "4  0206.653.946  Rijksinstituut voor Ziekte- en Invaliditeitsve...   \n",
      "\n",
      "           CorrectDomain             URL1Domain      URL2Domain  \\\n",
      "0  extranet.iglimburg.be           iglimburg.be  intergalva.com   \n",
      "1           proximus.com           proximus.com     proximus.be   \n",
      "2                 nbb.be                 nbb.be          nbb.be   \n",
      "3           interwaas.be  erfgoedcelwaasland.be         vvsg.be   \n",
      "4          inami.fgov.be          riziv.fgov.be   riziv.fgov.be   \n",
      "\n",
      "      URL3Domain               URL4Domain         URL5Domain           Labels  \n",
      "0   mapcarta.com       roamtechnology.com         geruro.com  [0, 0, 0, 0, 0]  \n",
      "1    proximus.be             proximus.com        proximus.be  [1, 0, 0, 1, 0]  \n",
      "2   nbbmuseum.be  openingsuren.vlaanderen       nbbmuseum.be  [1, 1, 0, 0, 0]  \n",
      "3    govserv.org           creditsafe.com            fsma.be  [0, 0, 0, 0, 0]  \n",
      "4  riziv.fgov.be         sociaal.brussels  desocialekaart.be  [0, 0, 0, 0, 0]  \n"
     ]
    }
   ],
   "source": [
    "merged_dataset = merged_dataset.drop(['URL', 'URL1', 'URL2', 'URL3', 'URL4', 'URL5'], axis=1)\n",
    "print(merged_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepreration for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class URLDomainDataset(Dataset):\n",
    "    def __init__(self, queries, domains, labels, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.queries = queries\n",
    "        self.domains = domains  # List of lists containing domains for each query\n",
    "        self.labels = labels    # List of lists containing binary labels for each domain\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)  # Ensure correct shape [5]\n",
    "        \n",
    "        # Assuming you are combining query with each domain\n",
    "        # This simplifies the example; adapt as necessary for your actual tokenization logic\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),  # Shape [max_len]\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),  # Shape [max_len]\n",
    "            'labels': labels  # Shape [5]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Prepare data for splitting\n",
    "X = merged_dataset[['SearchQuery', 'URL1Domain', 'URL2Domain', 'URL3Domain', 'URL4Domain', 'URL5Domain']].values\n",
    "y = np.array(merged_dataset['Labels'].tolist())  # Convert labels to a suitable format\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data\n",
    "queries = merged_dataset['SearchQuery'].tolist()\n",
    "domains = merged_dataset[[f'URL{i}Domain' for i in range(1, 6)]].values.tolist()\n",
    "labels = merged_dataset['Labels'].tolist()\n",
    "# Prepare the data for the Dataset instances\n",
    "train_queries = [x[0] for x in X_train]  # Assuming the first column of X is 'SearchQuery'\n",
    "train_domains = [x[1:] for x in X_train]  # Assuming the rest are 'URL1Domain' to 'URL5Domain'\n",
    "train_labels = y_train\n",
    "\n",
    "val_queries = [x[0] for x in X_val]  # Same assumption for validation data\n",
    "val_domains = [x[1:] for x in X_val]\n",
    "val_labels = y_val\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = URLDomainDataset(train_queries, train_domains, train_labels, tokenizer)\n",
    "val_dataset = URLDomainDataset(val_queries, val_domains, val_labels, tokenizer)\n",
    "\n",
    "# Initializing Dataset\n",
    "dataset = URLDomainDataset(queries, domains, labels, tokenizer)\n",
    "\n",
    "# Initializing DataLoader\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Define an appropriate batch size for your model and hardware\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\natha\\Anaconda3\\envs\\URLfinder\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "\n",
    "# Assuming you're using a GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=5,  # Assuming you have 5 URL domains to classify per query\n",
    "    problem_type=\"multi_label_classification\",  # Specify the problem type\n",
    ").to(device)\n",
    "\n",
    "# Initialize the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)  # Adjust parameters as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1724 [00:41<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "loss_func = BCEWithLogitsLoss()\n",
    "def evaluate_model_extended(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            logits = outputs.logits\n",
    "            preds = torch.sigmoid(logits).round().cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hammingLoss = hamming_loss(true_labels, predictions)\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "    precision_micro = precision_score(true_labels, predictions, average='micro')\n",
    "    recall_micro = recall_score(true_labels, predictions, average='micro')\n",
    "    \n",
    "    return hammingLoss, f1_micro, precision_micro, recall_micro\n",
    "\n",
    "epochs = 3 \n",
    "best_hamming_loss = float('inf')\n",
    "model_save_path = 'best_model_state.bin'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = loss_func(outputs.logits, batch['labels'].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    scheduler.step()  # Update the learning rate\n",
    "\n",
    "    # Evaluation step\n",
    "    hamming_loss_val, f1_micro, precision_micro, recall_micro = evaluate_model_extended(model, val_loader, device)\n",
    "    print(f'\\nValidation Metrics after Epoch {epoch + 1}:')\n",
    "    print(f'Hamming Loss: {hamming_loss_val}')\n",
    "    print(f'F1-Score (Micro): {f1_micro}')\n",
    "    print(f'Precision (Micro): {precision_micro}')\n",
    "    print(f'Recall (Micro): {recall_micro}')\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if hamming_loss_val < best_hamming_loss:\n",
    "        best_hamming_loss = hamming_loss_val\n",
    "        print(\"Hamming Loss improved, saving model...\")\n",
    "        torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./BERT_model/model\")\n",
    "tokenizer.save_pretrained(\"./BERT_model/tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URLfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
